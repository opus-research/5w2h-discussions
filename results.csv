MESSAGE,WHO,WHAT,WHEN,WHERE,WHY,HOW,HOW MUCH
['\ufeffHow about adding Dynomite and Elaticssearch to the image?'],0.0002526612245354188,0.9956091048919423,0,0.0002104785100662346,0,0.003841743419198652,0
"['Caught this in the compose logs, looks like there is something wrong with my elastic configuration:\\r\\n`conductor_1\xa0 \xa0 \xa0 | 8560 [main] ERROR com.netflix.conductor.server.ConductorServer\xa0 - Error starting embedded elasticsearch.\xa0 Search functionality will be impacted: file:/conductor-server-1.6.0-SNAPSHOT-all.jar!/es_template.json`. \\r\\nIf anything jumps out at you as to what I configured wrong, please let me know. Does this impact the use of dynomite? I will be taking a closer look at it tomorrow']",0.00046224865378991685,0.9872319115889423,0,0.007761082166996716,0,0.004100996442680195,0
['Can you update the .md file under the docs and submit PR?\xa0 I will do a gh-depoy from there.'],0.0012204666419100732,0.0361036864907405,0.00023061636168024487,0.9622935522457173,0,0,0
['Any reason this build got failed?'],0,0.0024110836144069178,0,0,0.9973487921755361,0,0
['@picaron This may not be required.\xa0 Have you tried configuring the workflow with workflow task marked as optional?\xa0 We have this feature in a pre-release version 1.7.2+.\\r\\n'],0.0018050761494973696,0.9976943235647596,0,0,0.0002830590570320687,0.00012982917630813674,0
"[""@v1r3n @picaron\xa0 Wouldn't it be better to make HTTP task extend the WorflowTask instead?""]",0.4586538467258229,0.523459560491577,0,0,0.00040522811601637705,0.016816858728401492,0
"[' actually I deleted the branch after you merged a recent PR because my changes were no longer compatible with the latest `master`, main reason being that the `COMPLETED_WITH_ERRORS` status was removed. I would be happy to create a new PR. Since this status `COMPLETED_WITH_ERRORS` no longer exists, how do to suggest to implement this? Should I simply set the HTTP task to `COMPLETED` or is there another better approach?']",0.004439278125957772,0.9844152243217039,0,0,0.00931672599584671,0.0015884625186007991,0
['@v1r3n any updates on supporting optional HTTP tasks?\\r\\n'],0.06850676359329792,0.9278768646079079,0,0.0004464307789559427,0,0.0027386085939214164,0
['@cquon will this change support ES 2.x?'],0.6452294041961046,0.3264143148771028,0,0.004649189851712773,0,0.019206398504940866,0
['@cquon thanks.\xa0 This could be a problem because we have few users including ourselves using it on 2.x and are not yet ready to move to 5.x. Is there a way to create a separate module for ES 5 and put this class in there?\xa0 That would allow folks to pick the right version.\xa0 Also I would suggest renaming the class to reflect that. Thoughts?'],0.008032596928555565,0.9550842714867837,0,0,0.03619099311812695,0.00038099206874366877,0
"['@v1r3n Yes that definitely makes sense.\\r\\n\\r\\nWhen you say different module do you mean a different top level project?\\r\\n\\r\\nThe legacy support for elasticsearch 2.4 has a dependency on ""org.elasticsearch:elasticsearch:2.+""\\r\\n\\r\\nwhile elasticsearch 5.']",0,0.11402611247715501,0.6478527928273253,0.2319022343024824,0,0.0020984197867706107,0
"['@cquon any chance you can create a separate module and put this there?\xa0 The reason is we have users using it over 2.x and there are no plans to push them to 5 for a while, so it will be required to keep both versions available till then.']",6.698720010723352e-05,1.0181685406353511e-05,0,0,0.9998827552637014,0,0
"['Sure will look into doing that first before moving to rest client.\\r\\n\\r\\nAs I mentioned earlier:\\r\\n\\r\\nThe elasticsearch 5.3.0 has a dependency on ""org.elasticsearch:elasticsearch:5.3.0"" and ""org.elasticsearch.client:transport:5.3.0"" which conflicts with the current ""org.elasticsearch:elasticsearch:2.+"" so only one will be picked up.\\r\\n\\r\\nDo you recommend under top level conductor directory having an elasticsearch2-client and elasticsearch5-client project for example, each with their own dependencies and then the build.gradle for redis-persistence use a specific one when building (""compile project(\':elasticsearch2-client\')"")?\xa0 So it can default to the existing elasticsearch configuration and one could just change it to (""compile project(\':elasticsearch5-client\')"") when wanting to change?']",0.00022883301786949154,7.839283696690628e-05,0,0.001762907754436939,0,0.9978744822189366,0
"[""@v1r3n could you consider merging this PR?\\r\\n\\r\\nWe really need HTTP tasks to support OAuth authentication\\r\\n\\r\\nIdeally we'd like to merge this feature here to avoid maintaining a separate dependency. I think this could be useful to others as well.\\r\\n""]",0.08678378357002207,0.9121308621516736,0,0,0.0008733316302277843,0,0
"[""Thanks for the pertinent comments @tommack \\r\\n\\r\\nRegarding your suggestions: \\r\\n\\r\\n1. You are right, maybe we could call this a JsonJqTransform ?\\r\\n2. I'm not sure about exposing the entire input data. That requires more complex processing in the query expression. A dedicated attribute gives some control of the data that gets passed to the processor.\\r\\n3. Good idea, I'll do that\\r\\n4. I'll fix that\\r\\n5. The issue I had is that using the JQ library, the result is always wrapped in a list even if the result of the query does not return a list. I think you need to know the type of result your query expression will be generating anyhow. We could implement something like you suggest but I feel the output will be more convoluted\\r\\n\\r\\n""]",0.011870351005369501,0.6474703875291299,0,0,6.648494678912422e-05,0.3405115613264421,0
"['1. done âœ… \\r\\n2. Maybe I\'m missing something but doing that, wouldn\'t the processor also receive the queryExpression parameter? I think its preferable to have some control on the data interpreted by the query no?\\r\\n3. done âœ… \\r\\n4. done âœ… \\r\\n5. The issue is that if you write some expression that does not return a list, somehow the Java jq lib still always wraps it in a list. For example, I was debugging my expression using the online tool (https://jqplay.org/) and it returns me the object I expect. But when the jq java library processes the result, it always wrap the result in a list. I think it\'s because you can optionally pass a list of transformations to the lib. That\'s why I need to ""unwrap"" that result. If you write an expression that returns a list, it will stay a list using the current code (because the jq lib will put that list in a list ðŸ˜± )\\r\\n\\r\\nPR : https://github.com/Netflix/conductor/pull/239']",0.0145378599694909,0.2543661791364473,0,0,0.06506649057511459,0.6629027177197641,0
[],0,0,0,0,0,0,0
"['This is a nice feature... is there any documentation for the usage? One of our API returns following JSON\\r\\n```\\r\\n{\\r\\n\xa0 \xa0 ""hits"": [\\r\\n\xa0 \xa0 \xa0 \xa0 {\\r\\n\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 ""_index"": ""mes$i$v2$lid_infor.daf.daf$mds_file$1.0$1561444341392"",\\r\\n\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 ""_type"": ""mds_file"",\\r\\n\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 ""_source"": {\\r\\n\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 ""LASTCHANGEDTS"": ""2018-11-06T10:01:10Z""\\r\\n\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 },\\r\\n\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 ""_id"": ""3_ax1"",\\r\\n\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 ""_score"": 4.491742\\r\\n\xa0 \xa0 \xa0 \xa0 },\\r\\n\xa0 \xa0 \xa0 \xa0 {\\r\\n\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 ""_index"": ""mes$i$v2$lid_infor.daf.daf$mds_file$1.0$1567159125621"",\\r\\n\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 ""_type"": ""mds_file"",\\r\\n\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 ""_source"": {\\r\\n\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 ""LASTCHANGEDTS"": ""2019-04-25T07:07:14.223Z""\\r\\n\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 },\\r\\n\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 ""_id"": ""3_ax2"",\\r\\n\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 ""_score"": 2.2275105\\r\\n\xa0 \xa0 \xa0 \xa0 }\\r\\n\xa0 \xa0 ]\\r\\n}\\r\\n```\\r\\nI used following task definition, but the workflow results in error. I tested JQ on jqplay.org and they work as expected.\\r\\n```\\r\\n{\\r\\n\xa0 \xa0 ""type"": ""JSON_JQ_TRANSFORM"",\\r\\n\xa0 \xa0 ""name"": ""transform"",\\r\\n\xa0 \xa0 ""taskReferenceName"": ""TransformSerchResult"",\\r\\n\xa0 \xa0 ""inputParameters"": {\\r\\n\xa0 \xa0 \xa0 \xa0 ""hits"": ""${search.output.response.body}"",\\r\\n\xa0 \xa0 \xa0 \xa0 ""queryExpression"": "".hits[] |=\xa0 { id: ._id, index: ._index, source: {created:._source.LASTCHANGEDTS} }""\\r\\n\xa0 \xa0 }\\r\\n}\\r\\n```\\r\\nAny suggestions ?']",0.0015754825069534365,0.08805259258588304,0,0.0007727551596319205,0,0.9093389791168087,0
['@v1r3n can you please review this very small change?\\r\\n\\r\\nit would be helpful for debugging HTTP tasks and also would allow to do some processing based on the reason phrase.'],0.9562603809546775,0.0436621886372378,0,0,0,4.2172044881556786e-05,0
['Hi @v1r3n\\r\\n\\r\\nCould you please review this very small PR ?\\r\\n\\r\\nit is a bit inconvenient having to update the `build.gradle` all the time when switching branches.'],0.17771536173103994,0.8219983427888073,0,3.175958463226653e-05,0,0.00021199433613361957,0
['@picaron can you send this against the `dev` branch?'],0.00028108849763789734,2.9909452213009773e-05,0,0.9996841297975001,0,0,0
"['How about not polling for tasks if the workers are busy?\xa0 So when the queue is full, the polling should be paused.\xa0 ']",0,0.003534151629821913,0,0,7.814187290832559e-05,0.9963094833578943,0
['@v1r3n\xa0 any comments?'],0.83523391135669,0.005342332091982539,0,0.15927345678874955,0,0,0
"[""what's the driving factor behind this change, if you don't mind my asking? DTO/DO separation tends to add a lot of complexity in my experience.""]",0.02494525388324857,0.6251294816891388,0,0,0.3460019043630876,0,0
"[""Hi @picaron \\r\\n\\r\\nI'm also interested in a SQL implementation for Conductor, so i was looking into your code.\\r\\n\\r\\nDid you try it with the Client provided?\\r\\n\\r\\nOn my tests it doesn't work, because it will always fail to Ack the task, due to the setUnackTimeout method always returning false.\\r\\n\\r\\nThanks,""]",0.013213538910462709,0.9865810221249818,0,0,0.00011519352705958602,5.700166956184499e-05,0
['Hi @luisnrkl \\r\\n\\r\\nI have added the missing implementation for `setUnackTimeout` and `processUnacks`\\r\\n\\r\\nCan you perform your tests again?\\r\\n\\r\\n\\r\\n\\r\\n'],0.507339842070402,0.4920702736832076,0,0,0,0.0003616051092438604,0
"['Hi @tommack \\r\\n\\r\\nI addressed all your comments and made a few improvements, can you please have another look when you get a chance?']",0.9343318818340408,0.044258612221097136,0.0043768749105125675,0,0,0.01676762223535928,0
['@v1r3n @tommack can you please review the PR again?\\r\\n\\r\\nWe addressed all comments raised by @tommack'],0.9974511305979643,0.0025338484639399967,0,0,0,0,0
['thanks @vikramsingh2104 \\r\\nwill you revert or I will create another PR ?'],0.9989685556048302,0.0005591196862193724,0,0.0002672459934338331,0,0.00011132574663217203,0
"['thanks @v1r3n \\r\\n\\r\\none more thing: the 1.8.0 release is currently unusable because of this issue, is it possible to make a new 1.8.1 release soon?']",0.009458139362391095,0,0.164945559821442,0,0.7333826064114177,0.08946050822168493,0
"['Hello @vikramsingh2104, do you have any update about this review? :)']",0.8123181704811897,0.02983606148875414,0.12309518021131999,0.0289553005053628,0,0,0
"['Hello @v1r3n @vikramsingh2104, do you have any news about this PR? :)']",0.9957004476549702,0.0017982282578805665,0.001989147746637566,0.00042404426906936796,0,0,0
['This could lead to huge reads - its quite possible that number of tasks over the period grows to large size (millions).\\r\\n\\r\\nHow about using indexing system in elasticsearch to get the tasks?\\r\\n\\r\\n@math29 what do you think about the above?'],0.00028726828152659455,3.445060496469544e-05,0,0,0.00011369456040464886,0.9995557247195026,0
"[""We don't wanna use elasticsearch on our side for now.\\r\\n\\r\\n@v1r3n what about using SSCAN for getTasks methods (adding 2 parameters to this method for pagination)?""]",0.9964915012158768,0.003349687235637432,0,7.873372250285734e-05,0,3.61408953000884e-05,0
['thanks @v1r3n \\r\\n\\r\\nare you going to create another 1.8.2 release candidate?'],0.9755434545362885,0.01889651056788228,0,0.0024241144416201855,0,0.0018165303472529263,0
['@v1r3n @vikramsingh2104 can you review/merge this one-liner PR?\\r\\n\\r\\nwe found out the latest Flyway version is not compatible with Percona XtraDB Cluster (a MySQL HA solution) in strict mode.\\r\\n\\r\\nso the only fix currently is to downgrade the Flyway v'],0.013233272194906887,0.9853838995550731,0,0,0,0.0009315152468447103,0
['@heajuenhwang can you send this PR against the `dev` branch?'],0.9998260335839122,2.63043561346475e-05,0,0.00010998550209439411,0,1.5459250437056762e-05,0
['@heajuenhwang can you open this PR against the dev branch?'],0.9995880040936315,0.00017174615745650196,0,0.00012969668836573003,0,4.675772851798583e-05,0
['@v1r3n @vikramsingh2104 any news on releasing a new RC soon?'],0.9981757118401783,0,0.0004251641195222749,0.0011128135955557404,0,0.00026370091361792975,0
['question on the 1) above - I am trying to understand the need for a separate redis-queue-persistence module.\xa0 How is it different from the queues already being used?'],0.0012542706885225637,0.07275444319137439,0,0,0.9257446762014833,8.875617984292404e-05,0
"['@v1r3n @vikramsingh2104 @pctreddy @cyzhao , any idea when this will be merged? Our production release depends on this. Thanks!']",0,0,1.6009888425988757e-05,0,0,0,0
['@pctreddy can you verify that you can create releases with upgraded gradle?'],0.8546140119418008,0.14483222144410654,0,0.00012479439685899842,0,0,0
['@nsiitk can you send the PR against the `dev` branch?'],0.7586225912375377,0.00644818360153322,0.002045900181067258,0.23217225601420127,0,0,0
"['Hi @v1r3n @cyzhao, any updates?']",0.9684428516004789,0,0.0038611778483198115,0.026443439115772075,0,0.0008412198644455154,0
['@huangyiminghappy can you send the PR against `dev` branch?'],0.9998818017459641,2.7428772617655948e-05,0,3.9384232977070945e-05,0,2.1524300613538988e-05,0
['@huangyiminghappy can you remove the `conductor.iml` and send the PR against `dev` branch?'],0.9952945221129121,0.004537601547623708,0,0.00013056491786330053,0,0,0
"[""I'm unable to reproduce the failure from CI. Any pointers or is that unrelated?""]",0.001206752514174135,0.9745101056510391,0,0,0.00048365597472772387,0.023121744127097887,0
['@rkrsingh can you open this PR agains the `dev` branch?'],0.9999269756164101,1.164089766090524e-05,0,2.0323412730912034e-05,0,2.0249509235284456e-05,0
['@mashurex do you want to add the rest of the stuff (module etc) and update the PR so it replaces the existing implementation?\\r\\n\\r\\n@picaron @pctreddy'],0.4646533478054546,0.5316618563977361,0.0021478597451959004,0,0,0.0010840177516910664,0
['@picaron and @v1r3n is there any chance we can get this reviewed and merged soon? Keeping it up to date and conflict free is getting a bit troublesome.'],0.8137356988432211,0.00039409419643285544,0.15508689596842026,0,0,0.03025399612271203,0
"[""@mashurex I agree, let's get this done asap. I think we are very close. Latest code looks good to me overall and all tests run fine but there is one remaining issue: the existing flyway migration script was modified so everyone upgrading will be getting a flyway conflict. Could you please put the changes in a new `V2__xyz.sql` migration file? You can for example add a `V2__update_queue_message.sql` file with an alter table statement in it. When flyway runs on a mysql database with the conductor schema already initialized, it will detect that that initial V1 migration was already done and it will apply the missing V2. Have a look at this page: https://flywaydb.org/getstarted/how ... it explains very clearly how flyway works. Ideally, I'd like to test running my workflows using your PR to make sure everything still works before we proceed with the merge.""]",0,0.020695183229818063,0.9060674614043643,0,0.03515444186013245,0.03588008761158748,0
"[""Sounds good. I will get the migrations fixed ASAP.\\nOn Sat, Apr 14, 2018 at 09:24 Pascal Chouinard <notifications@github.com>\\nwrote:\\n\\n> @mashurex <https://github.com/mashurex> I agree, let's get this done\\n> asap. I think we are very close.\\n>\\n> Latest code looks good to me overall and all tests run fine but there is\\n> one remaining issue: the existing flyway migration script was modified so\\n> everyone upgrading will be getting a flyway conflict.\\n>\\n> Could you please put the changes in a new V2__xyz.sql migration file? You\\n> can for example add a V2__update_queue_message.sql file with an alter\\n> table statement in it.\\n>\\n> Ideally, I'd like to test running my workflows using your PR to make sure\\n> everything still works before we proceed with the merge.\\n>\\n> â€”\\n> You are receiving this because you were mentioned.\\n> Reply to this email directly, view it on GitHub\\n> <https://github.com/Netflix/conductor/pull/426#issuecomment-381340988>,\\n> or mute the thread\\n> ""]",0.279065432379409,0.6208839441131485,0.09847222010231414,0.0006389262659463312,0,0,0
"['@mashurex is it possible to squash your commits into a single one? \\r\\n\\r\\nonce merged into the target repo, it would keep the commits history a bit cleaner']",0.3844568961352421,0.07552046498901374,0.022382217432926742,0,0,0.5140707536421423,0
['@v1r3n are ok with merging this?'],0.9993959178311462,4.596542842560745e-05,0,3.3374647603374256e-05,0,0.000470108236193211,0
"['wonderful, thanks @mashurex !\\r\\n\\r\\n@v1r3n can you merge this soon? to avoid our friend further rebasing his PR and managing conflicts ðŸ˜€ \\r\\n\\r\\nthanks in advance!\\r\\n']",0.7610547034987142,0.006889649955253036,0.22255938814044793,0.005172771813453932,0,0,0
"[""@v1r3n I'm not familiar with the `dependencies.lock` how can that be updated? \\r\\n\\r\\nis there a special command or we can just remove all references to sql2o from there?""]",0.011338737709199876,0.9884137683771126,0,0.00010527327181097174,0,9.448466611160133e-05,0
['seems @mashurex has addressed all the latest comments\\r\\n\\r\\ngood to merge @v1r3n ?'],0.7306145132362192,0.2684092261091971,0,0,0,0.00040433116517779604,0
"[""@pctreddy I'm sorry for contacting you like this but since you seem to be very active here I would like to know if there is anything I can do to get feedback for my PR?""]",0.9986654128545729,0.0002826107637929159,0,0.0006553507299939327,0.00020923281052451814,0,0
"['@v1r3n , @pctreddy\xa0 - anything else that needs to be done before this pull-request can be merged?']",0.4745457254653461,0.5208327271043581,0,0.00438317908228018,0,7.378065412130965e-05,0
['Any update about this PR? Could it be merged?'],0.030696289343010337,0.9654530991898301,0.002203438244874188,0,0,0.0010549470530457439,0
['can you help me review the pr in https://github.com/Netflix/conductor/pull/503?\xa0 Thanks'],0.011364866140154594,0.0018276468753888127,4.817935698300639e-06,0.9867874759364957,0,0,0
['@hunterford\xa0 Can you assign reviewers?'],0.8214668297358921,0.17832140693332493,0,0,0,0,0
['@zsimic Can you merge this branch?'],0.977708601324754,0.02171134161289806,0,0.0003434609965192291,0,0,0
['@pctreddy can you resolve the conflict so that can be merged?'],0.9957211345423653,0.00414500203268727,0,0,0,2.1004759564344297e-05,0
['Is there any document on the size limit of task output/input?'],0,0.5044247237171186,0,0.14994213889544425,0.02405546195258359,0.29753539091884856,0
"['hi ,apanicker-nflx ,can you help me review the https://github.com/Netflix/conductor/pull/496\xa0 pull request? thank you']",0.9971602943135477,0.002789344937898147,0,0,0,0,0
['@cyzhao\xa0 Can you review this changes?'],0.9908136016639175,0.009068966213654944,0,2.4901861456466173e-05,0,0,0
['@vmg can you take a look at the build issue?'],0.9220024206455285,0.07792251838765754,0,2.3204960209750792e-05,0,0,0
"['Hi @cyzhao , when you get a chance can you please review the PR?']",0,0,0.00030285512563950096,0,0,0,0
['@jvemugunta @cyzhao @v1r3n will it be possible to spend sometime on the PR and suggest if this is the right approach ?'],0.9989026493300779,0.0010127984874596441,3.586061284500882e-05,0,0,1.5666842838106936e-05,0
['@mstier-nflx can you please take a look at this PR? \\r\\n\\r\\nThanks.'],0.9998331845852388,0.00014287734829126664,0,0,0,6.804985858277743e-06,0
['@hunterford @mstier-nflx How do I get this merged?'],0.19818371994314943,0,0,0,0,0.8017345267021284,0
['@hunterford Any thoughts on this merge request?'],0.9987804001171781,0.001174638310057915,0,0,0,0,0
['@mstier-nflx or @hunterford : Can you please merge this PR?'],1.795850031443677e-05,0.012568218408963923,0,0,0,0,0
['Hi @v1r3n @cyzhao \\r\\n\\r\\nCan you please review the pull request? \\r\\n\\r\\nThank you\\r\\n\\r\\n\\r\\n'],0.1452089702381498,0.8547609070751049,0,0,0,0,0
['@hunterford I resolved the conflicts. Can you please re-approve and merge these changes when you get a chance?\\r\\n\\r\\nThanks.'],0.8963185676014683,0.08532262664896618,0.002887200302104195,0,0,0.01440654444925859,0
['@mstier-nflx or @cyzhao can you please merge this so I can avoid further conflicts?\\r\\n\\r\\nI appreciate it.'],0.9995964604739859,0.00013014335295680486,0.00011044498878712537,0,0,7.346841294260006e-05,0
"['@hunterford\xa0 @cyzhao @mstier-nflx\xa0 any feedbak, thoughts, etc? I have the redux upgrade behind this one done, and I am working on the React 16.4 and Router upgrade along with some elastic dash-boarding behind this and my other pull request.']",0.8274611181208388,0.17213555536140665,4.050970430844846e-05,0.0003221497688186782,0,0,0
['@pctreddy @cyzhao can you review this?'],0.7442827470403571,0.25541617636925934,0,0,0,0.00010500075682243634,0
"['@hunterford can I get a review on this one? I have the Redux / Routing / React upgrades almost done, but this is a base for those changes.']",0.9972800227595221,0.002519521188690101,0.00014149007283290356,0,0,0,0
"['I had a look to the travis log but I am not able to reproduce the error on my machine. `MySQLNonTransientConnectionException` can be thrown for several reasons, any possibility we can increment the `log level on travis`, use a `gradle scan` or add the full stacktrace?\\r\\n']",0.0004788026737609478,0.03450118181583779,0,0,0.3683380530758136,0.5959805480198446,0
"['@apanicker-nflx should I change the target branch? The major version would be a good time to put in the React 16+, React Router and Redux upgrades too.']",0.1369130398925442,0.8620388842474936,0,0,0.0005964376788041172,0.0001943256884651553,0
"['Hi @gorzell \\r\\n\\r\\nDid you look at the class MySQLWorkflowModule I have made changes few weeks back?\\r\\nIt addresses all of the properties you mentioned. I like your idea of separating MYSQL Configuration from code. However, Consider having\xa0 setMaximumPoolSize and setMinimumIdle settings. These are very critical for High Volume applications. \\r\\n\\r\\nHow can I test your code? what branch/repo I should pull from?\\r\\n\\r\\n\\r\\n```\\r\\nHikariDataSource dataSource = new HikariDataSource()']",0.013346287802929789,0.9844586655552031,0,0,0,0.0020780311417753503,0
['Can we get this reviewed for a merge?\xa0 I too have to maintain this change in a fork\\r\\n\\r\\nAlso looks like PR could use a rebase'],4.687530814196744e-05,0.9999447358231209,0,0,0,0,0
"['Hello, Can someone please review and merge the code?']",0.014861653113775134,0.9850579530076788,0,0,0,2.0089927257002698e-05,0
['@mashurex - Can you review this code? The current code has race conditions and I have addressed all of them.'],3.348905128052493e-05,0.0016595901577806146,0,0,0,0,0
"[""> @jvemugunta I incorporated your changes into my previous PR, the only difference is how I've setup the test harness. If you merged in my PR branch to yours I could just close it and we can use this PR to post both of our changes.\\r\\n\\r\\nok @mashurex , I will merge your changes into mine so that we work with one code base.\\r\\nIs all your code in this repo\\r\\nhttps://github.com/mashurex/conductor/tree/fix/754_mysql_timeout?\\r\\n ""]",0.9999425902750013,2.6557058200866056e-05,5.504277724168658e-06,1.3723649122539282e-05,0,0,0
"['@mashurex , Your branch is merged into this PR. Please review and let me know what you think. @mashurex and @picaron - Can we revisit the design of queue_message table? https://github.com/Netflix/dyno-queues/blob/dev/dyno-queues redis/src/main/java/com/netflix/dyno/queues/redis/RedisDynoQueue.java If you look at the push, pop and processUnacks methods - It separates unackTime time value from myQueueShard time value where as we are merging these two into deliver_on column. Can you design our implementation similar to REDIS? We are doing select and updates on queue_message table - This would require locks when selecting and updating data with high volume How about inserts and deletes - This way the code will be clean and runtime with MYSQL may be better. Your thoughts?']",0.0003231154074340874,0.9996560143266428,0,5.461052615864908e-06,0,1.1367160699366945e-05,0
['@mashurex @jvemugunta can you confirm that https://github.com/Netflix/conductor/pull/757 can be closed given it is merged into this one? also let us know when this PR is good to go then we will review and merge accordingly.'],0.07055756341360213,0.012489974456035807,0.9164679942730889,0.000248312630212491,0,0,0
"['Hi @joshlreese , Which branch your\xa0 code is running with?\\r\\n\\r\\nON DUPLICATE KEY UPDATE would not create a deadlock. \\r\\n\\r\\nYou probably are seeing the deadlock because of \\r\\n\\r\\nhttps://github.com/Netflix/conductor/blob/2c419cff934959f72bdb48284b433c6d26f52a24/mysql-persistence/src/main/java/com/netflix/conductor/dao/mysql/MySQLQueueDAO.java#L210\\r\\n\\r\\nThis is because of Pessimistic we implemented, but later realized this would cause deadlocks under heavy load. With this fix we are going to change as Optimistic lock. This would avoid the dead lock situation.\\r\\n\\r\\nIf you can recreate the deadlock, I want to see the behavior. can you run this command\\r\\nSHOW ENGINE INNODB STATUS \\G\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n \\r\\n\\r\\n\\r\\n']",0.03846282840707261,0.76448161675818,0,0.15165728617616073,0,0.04508637430328027,0
['@apanicker-nflx - why would you think this happens on Travis-CI with GITHUB?\\r\\nI have installed Travis-CI docker image on my local machine and it works with no issues? Do you think if these tests *ServiceTest are running in parallel resulting into some k'],0.00013684618501423398,0,0,0,0.9993225831504674,0.00020572957382624797,0
['@apanicker-nflx - I think the problem is with WorkflowLegacyMigrationTest and WorkflowServiceTest sharing the same TestRunner class resulting into this race condition. I have made WorkflowLegacyMigrationTest as a simple test independent of TestRunner. the build succeeds. Can you look and tell me if you have insight on the rootcause?'],0.006401504103862391,0.9935430110265778,0,9.203773453609922e-06,1.4243427296577847e-05,0,0
"['Is the removal of ""SELECT ... FOR UPDATE"" in PEEK_MESSAGES on purpose?\xa0 Without it, this causes the occasional duplicate processing of tasks when running multiple instances of conductor.\\r\\n\\r\\nI must say that it is not as easy as just add ""FOR UPDATE"" to the select statement, because that ends up essentially locking the whole table if you don\'t provide an appropriate LIMIT.\xa0 The default is to ""LIMIT 10"", and if there are less than 10 records, this will prohibit not only updates to the locked rows, but also other rows as well as inserts to the table as a whole (MySQL 5.7).\xa0 I\'ve circumvented this by doing a SELECT COUNT within peekMessages() and using the resulting count instead of the passed in value if it is less.\xa0 Still a little bit of a race condition, but better than the alternatives.\\r\\n\\r\\nIf there\'s not a reason, I can submit a patch, or feel free to convert my description into code.\\r\\n']",0.006747751099876161,0.9318628702936518,0,0,0.051218990884295754,0.00986381074243224,0
"['@jvemugunta can you revert format changes, or mark code changes related to the fix?']",0.9850204801458063,0.011960568477807326,0,0,0,0.0027319424572374625,0
"['I am using Eclipse as an IDE and I am not sure how the formatting is the changing. Is there a . settings we can use for all developers who are using eclipse. This way the indentation and formatting is consistent for all developers \\n\\nSent from my iPhone\\n\\n> On Sep 23, 2018, at 11:33 PM, Charles Zhao <notifications@github.com> wrote:\\n> \\n> @jvemugunta can you revert format changes, or mark code changes related to the fix?\\n> \\n> â€”\\n> You are receiving this because you were mentioned.\\n> Reply to this email directly, view it on GitHub, or mute the thread.\\n']",0.00023495771410264558,9.366996166492659e-05,0,1.686793627416516e-05,0,0.9996535119408569,0
"[""@cyzhao we also met this problem the task status marked as 'IN_PROGRESS' and not execute anymore, I saw this issue here. So I want ask the question is @jvemugunta applied the changes with two classes change, but I saw only the WorkflowExecutor.java merged to release1.12.3 branch, we met that issue may caused the other class(ServerModel.java) not merged to this release branch?""]",0.029891378041058677,0.9692739150181239,0,0.00031499304460546267,0.00044483461347111735,0,0
"[""Hey @piokania Thanks for the PR. Couple of questions / concerns: Why is this GuiceContainer injected to SwaggerModule? Also, I see the builds have failed for this PR.\\r\\n\\r\\nAs it has been a while, I'm closing this PR for now. But, if you're still interested in merging this, please let me know and I'd be glad to reopen.""]",0.00016857951843420648,0,0,0,0.9997677621813781,0,0
['@cyzhao Should we record all the exceptions sent from conductor?'],0.008786037194981968,7.782311989807877e-05,0,0,0.0003274975447053407,0,0
"['Better yet, all the calls go through `executeWithRetry` method of `RedisDynoQueue` in `dyno-queues-redis` repository. Shall we submit a PR to that repo to include metrics in all exceptions thrown?']",0.015782650687649606,0.9713005784810549,0,0.0053001365214058035,0,0.007411515794000108,0
"['So, we are currently running this in our testing environment and will push up to production by the end of this week if things look good. Not sure if there is anything additional that I should do in this PR?\\r\\n\\r\\nAny other comments? Otherwise, it would be nice to get this code in.']",0,0,2.754727330110876e-05,8.988630176587334e-05,0,0,0
"['@kishorebanala any word on being able to patch this into 1.12? I also created another PR to patch this into Conductor 2.0. The pattern is a little different, but I was able to port most of what I wrote here to there. https://github.com/Netflix/conductor/pull/843']",0.2960162877469005,0.650021233306898,0.03343003271294685,0.016701973381227076,0,0,0
"['@bitbier Did you guys meet the timeout problem, when I running my workflow with this REST client, I found there is something weird, each index operation there will be a request to perform set the timeout, but sometimes it\'s failed, which will caused the workflow index failed then the status of the whole workflow will not synced to ES, please help to take a look, anybody can give suggestion on this problem?You can see below debug log, the workflow failed index to ES seems caused by the ?timeout=1m] failed, I\'m just wonder do we need config the timeout from the REST Client config part like:RestClientBuilder builder = RestClient.builder(new HttpHost(""localhost"", 9200)).setRequestConfigCallback(new RestClientBuilder.RequestConfigCallback() { @Override public RequestConfig.Builder customizeRequestConfig(RequestConfig.Builder requestConfigBuilder) {return requestConfigBuilder.setConnectTimeout(5000).setSocketTimeout(60000);}}).setMaxRetryTimeoutMillis(60000);**Debug log:**2343004 [pool-2-thread-1] DEBUG 2018-11-12 16:29:30,554 org.elasticsearch.client.RestClient\xa0 - request [PUT https://vpc-listing-workflow-es-dev-34omml7hqotv3uwxuyovolawgm.ap-northeast-2.es.amazonaws.com/conductor/workflow/c7b33834-d04c-4454-a870-0d212fbecb17?timeout=1m] failed2343004 [http-bio-8080-exec-8] DEBUG 2018-11-12 16:29:30,554 com.netflix.conductor.common.utils.RetryUtil\xa0 - Attempt # 1, 1 millis sincefirst attempt. Operation: indexWithRetry, description:Indexing workflow: c7b33834-d04c-4454-a870-0d212fbecb172343158 [pool-2-thread-1] DEBUG 2018-11-12 16:29:30,708 org.elasticsearch.client.RestClient\xa0 - request [PUT https://vpc-listing-workflow-es-dev-34omml7hqotv3uwxuyovolawgm.ap-northeast-2.es.amazonaws.com/conductor/workflow/c7b33834-d04c-4454-a870-0d212fbecb17?] failed2343158 [http-bio-8080-exec-36] DEBUG 2018-11-12 16:29:30,708 com.netflix.conductor.common.utils.RetryUtil\xa0 - Attempt # 3, 6870 millis since first attempt. Operation: indexWithRetry, description:Indexing workflow: c7b33834-d04c-4454-a870-0d212fbecb17']",0.06941221627104523,0.8090495462333669,0,0,0.056525342588781495,0.0573582438180276,0
['Can you please close this PR as well? #478'],0.20030176992078577,0.09257810614539999,0.530343404094326,0.11562236183176394,0,0,0
"['Hi @davidwadden can you run conductor + dynomite + elasticsearch using docker-compose with the configurations that you provide in this PR? I lost some time trying to run conductor + dynomite + elasticsearch in my machine using the docker-compose after making some changes to it but conductor is much more slower than when I run it without dynomite and elasticsearch in different containers, as I mentioned here: #839.\\r\\n\\r\\nYou just need to apply these changes to run everything correctly or have you done other changes?\\r\\n\\r\\nThanks!']",0.1534602351365977,0.7108175233563244,0,0.001518788957766685,0,0.13349548792868654,0
['@cyzhao Since ES transport client is deprecated in future releases : https://www.elastic.co/guide/en/elasticsearch/client/java-api/master/transport-client.html Is there any plans of merging this / completing this PR ? @davidwadden are you still working on the changes ?'],0.98669393390448,0.008552351539403341,0.000193663779075679,0,0.004400047247708528,0,0
['@cyzhao how can I fix the coverage error?'],2.462124408500521e-06,0,0,0,0,0.9999907469518097,0
[],0,0.12845212656210414,0,0,0,0,0
['Do you have an example of the configuration and redis deployment you would use with this?'],0,8.687762836289985e-05,0,0.9977911654732093,0,0.0019821669768065654,0
"['Question: we are looking to extend the workflow task definition to include a human readable description, for visualisation of the workflow process. Would this validation fail if unknown properties are present? In the example below we add the `description` property, which is not part of the current schema.\\r\\n\\r\\nFor example:\\r\\n\\r\\n```\\r\\n tasks: [\\r\\n\xa0 \xa0 \xa0 \xa0 {\\r\\n\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 name: ""calculate_feasibility"",\\r\\n\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 taskReferenceName: ""calculate_feasibility"",\\r\\n\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 description: ""Something human readable goes here, and this is not in the current Netflix schema"",\\r\\n\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 type: ""SIMPLE"",\\r\\n\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 inputParameters: {\\r\\n\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 appId: ""${workflow.input.appId}""\\r\\n\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 }\\r\\n\xa0 \xa0 \xa0 \xa0 }\\r\\n]\\r\\n```\\r\\n\\r\\n']",0.0021479007697088152,0.9972175399376974,0,0,0.0003033637431510741,0.0002298138326642101,0
[],0,0,0,0,0,0,0
['Would this catch https://github.com/Netflix/conductor/issues/879? Input parameters with spaces in them.'],0.0006272207537190465,0.006046981845065098,0,0.9869842738088095,0,0.00610329798881157,0
"[""> Would this catch #879? Input parameters with spaces in them.\\r\\n\\r\\nDon't cover this case. Will add it in this PR.""]",2.4442772231487256e-05,0.9998676256222612,0,0,0,9.631639542665902e-05,0
['Thanks for the PR @davidwadden we tested it and it seems to work. @cyzhao / @davidwadden do we have plans to merge this ? Will be happy to help out if there is additional work required here.'],0.9993654426167479,0.0003261109164353247,0,0,0,0.00011302144256965277,0
"[""@haoch This is a great start, but seems out of place. I'm not sure if this PR helps create new Workflow from UI. If so, that's great. But ideally, the flow should be a Create task and workflow button in Metadata section of UI, and a form with all the options, which on submit would register a Workflow definition. If this is not what you've intended to do, can you please describe you plan? Also, I'm closing this for now as it has been a while. Please feel let me know if you're still working on this, and I'd be glad to reopen.""]",0.06908970819771845,0.8240368163783309,0,0,0.026920128521921816,0.07818623264992856,0
"['@kishorebanala as we are using conductor and needs the function for daily operations, could you add a similar feature?']",0.19023833465387513,0.8094630195369867,0,7.322658645515154e-05,0,0.00015452711119970833,0
"['hello @cyzhao @apanicker-nflx, apart from the specific implementation of the status listener that we needed to develop here, is it possible to review the possible fixes for some issues I found and added in this pull request?']",0.9842325072359956,0.0156555467479573,2.9591718978956898e-05,4.217929909712104e-05,0,0,0
"['Hello @Ismaley and @cyzhao,\\r\\nAny news about this feature? Is it still under review?']",0.5055057832167558,0.0589307472449349,0.4340958398992412,0.0007766575797762402,0,0,0
"['Hello @cyzhao, and @apanicker-nflx. We are using conductor in our company as orchestration engine and we needed support to elasticsearch 6. As discussed in the issue #889, i adapted the es5 module to support es6, and removed es5 support. Could you please review this pull request ?']",0.015228615839805937,0.9844806162747857,0,0.00010191585868117106,0,9.729324457606763e-05,0
"[""@apanicker-nflx thank you for the quick reply. As discussed in the issue #889 and said by @ahai1980, i thought that only ES6 would be supported in new versions of conductor. If that's not the case, i will rollback ES5 module and do what you suggested. But i have some questions: by standalone ES6 module did you mean a module that is only pluggable at build time? Because in early versions of conductor it was setup on load of application and I don't know how to properly do that. The ES6 client is not compatible with ES5 and I can't have both ES5 and ES6 clients jars in the classpath. Any suggestions ?""]",0.0057537377032874934,0.9940402110213309,0,0,2.045780675320158e-05,0.0001606944214318137,0
"['> Hey @srabenhorst , this is great. But, it would break backwards compatibility for users already using `registerTaskDef`. Instead, this method can be marked deprecated, and another `updateTaskDef` method can be added, with comments asking the users to use `updateTaskDef` hereon.\\r\\n\\r\\nIs it better like this? I hope I deprecated it properly.\\r\\n']",0.3905221848189363,0.4113980095542286,0,0,0.09864836701207388,0.09765169147469475,0
['Looks like this is related to this issue #573 \\r\\n\\r\\nWe are using conductor in our company as orchestration engine and we were considering building something exactly like @X-Ultra did here (many thanks!) to do some simple stuff during workflow execution. What do you guys think of this? @apanicker-nflx @cyzhao @kishorebanala'],0.027407636501079902,0.36120941206970425,0,0,0.6110113462595896,0,0
"['> @huangyiminghappy Thanks for resubmitting the PR. Can you fix the code coverage errors please.\\r\\n\\r\\nthank you review. @kishorebanala i can not found the\xa0 parametersUtils test case, do not know add the case in which test class,could i add a new test class to cover the uncoverd code?can you help me where i can add the test method?']",0.038377700661770334,0.9612413662472684,0,0,0,0.00027706317754581326,0
['@apanicker-nflx Can you review the PR again?'],0.934912319524791,0.0645370113740047,0,0,0,0.00021216569677759298,0
['@mstier-nflx and @hunterford I have resolved the conflicts. Could you take a look? Thanks.'],5.184728916012932e-06,0.00011727194546578201,0,0,0,0,0
['I am not seeing a lazy-debug-legacy@0.0.3 in the npm repository.\xa0 Where did you find that version?'],0,0,0,5.849327228518702e-07,0,0,0
"[""I tried other ways, but this seem to be the ideal way the solution works. How about calling it as a COMMAND task if you don't want it to be called it as EVENT_WAIT?""]",1.6013508558825225e-06,7.407993041079161e-06,0,0,0,0.9999837140256179,0
"[""> I used the existing implementations of EventQueueProvider and ObservableQueue to add the support for AMQP queues.\\r\\n> I've used the version 3.7.13 of RabbitMQ server.\\r\\n> Are you interested ?\\r\\n> Thanks for all the work you do and you done on Conductor :)\\r\\n\\r\\n@mekahell Can you consider exchange and routing key for publishing instead of using a queue name? I would consider the format for sink as amqp:exchange_name:routing_key, but for consumption it would be a queue name in the format exchange_name:queue_name""]",0.18564060604303406,0.7596912866502776,0,0.0015712301993262507,0,0.05294925907237744,0
[],0,0,0,0,0,0,0
['Can you please point me to the specification/documentation for this format to specify queue and exchange?\\r\\nAlso the URI path definition from your format needs a defined queue name. When you deal with exchange and routing key for publishing then you may not care about the queue name and infact you would never bind the code or configuration to a specific queue.'],0.00016249164059662502,0.9971396597904983,6.036156479682364e-05,0.0025765319235377497,0,0,0
"['@jkaipa I\'ve made some changes according to your review. There are 2 types of URI scheme for AMQP queues : - **amqp-queue** for queue - **amqp-exchange** for exchange. You can append parameters to the URI to set the following properties:- ""exchangeType"" : type of exchange topic, fanout, direct,...- ""routingKey"" : the routing key- ""deliveryMode"" : the delivery mode of message to publish- ""durable"" : is queue durable ? true by default- ""exclusive"" : is queue exclusive? false by default- ""autoDelete"" : is auto delete enabled ? false by default- ""maxPriority"" : Enable the priority feature on queue and set the max priority For more details about the query parameters please see the Enum class com.netflix.conductor.contribs.queue.amqp.AMQQueryParameters and the unit tests.']",4.997413431944895e-05,7.207575011969847e-06,0,0,0,1.7252060022894946e-05,0
"['> @jkaipa I\'ve made some changes according to your review.\\r\\n> There are 2 types of URI scheme for AMQP queues :\\r\\n> \\r\\n> * **amqp-queue** for queue\\r\\n> * **amqp-exchange** for exchange\\r\\n> \\r\\n> You can append parameters to the URI to set the following properties:\\r\\n> \\r\\n> * ""exchangeType"" : type of exchange topic, fanout, direct,...\\r\\n> * ""routingKey"" : the routing key\\r\\n> * ""deliveryMode"" : the delivery mode of message to publish\\r\\n> * ""durable"" : is queue durable ? true by default\\r\\n> * ""exclusive"" : is queue exclusive? false by default\\r\\n> * ""autoDelete"" : is auto delete enabled ? false by default\\r\\n> * ""maxPriority"" : Enable the priority feature on queue and set the max priority\\r\\n> \\r\\n> For more details about the query parameters please see the Enum class com.netflix.conductor.contribs.queue.amqp.AMQQueryParameters and the unit tests.\\r\\n\\r\\nThe next point is about the consumption. Is there a reason to go with basic get vs basic consume? how about doing a basic consume like below? (I didn\'t add additional params such as prefetchCount etc., for now). Conductor would be running much faster with its task execution. If the messages are getting into rabbit success or failure queue at faster rate, basicConsume might deliver better performance.\\r\\n\\r\\n```\\r\\n\\r\\n @VisibleForTesting\\r\\n public void receiveMessages() {\\r\\n try {\\r\\n DeliverCallback deliverCallback = (consumerTag, delivery) -> {\\r\\n Message message = new Message()']",0.01242124830348966,0.03514947840337009,0,0.001046600067009788,0,0.9512856019643992,0
['@apanicker-nflx any news here?'],0.994215987184239,0.004613795231673793,0.0005554108765555588,0.0004528715621252672,0,0,0
"['Hey @mekahell, we had to revert this PR as the build have been failing with couple of tests. Do you mind taking a look? https://travis-ci.org/Netflix/conductor/builds/568130006']",0.14616499392122725,0.7181934408459208,0.05302638283215804,0,0.07197852272148317,0,0
"[""@apanicker-nflx Thanks for the feedback! ... I'll make the changes soon. For the first point of removing the refactoring for`StartWorkflowParameters`. I do agree it doesn't belong here, but honestly, I wasn't able to proceed with the change without doing this refactoring. There were about 10 functions named `startWorkflow` that rely on parameters overloading, each has >5 parameters. It won't be really easy to undo this refactoring now. Does this sound like a good reason for keeping this refactoring in the same merger request?""]",0.010413622231102215,0.9548470637459614,0,0,0.023154449821725768,0.011217666245862203,0
['@mohelsaka Can you please provide an update on this PR? Thanks'],0.880261937104514,0.009817811921775221,0.08613919023234688,0,0,0.01918589318389965,0
"['@apanicker-nflx In which version, this ""Support of Priority"" will be available?']",0,0,0.001031135455437144,0.2201329493340224,0,0,0
[],0,0,0,0,0,0,0
['@apanicker-nflx Could you have a look when you have time?'],0,0,8.698229202232836e-05,9.239691198045926e-05,0,0,0
['@andrea11 Thanks for the contribution. The PR looks good as a whole. Can you please also add an integration test to the `test-harness` module in the `AbstractWorkflowServiceTest` class?'],0.008945335998592263,0.990979708147032,0,2.9191470427772612e-05,0,0,0
[],0,0,0,0,0,0,0
[],0,0,0,0,0,0,0
"[""@kishorebanala and @apanicker-nflx, thanks for reviewing this PR. @kishorebanala, I liked your idea! Just to make sure, are you suggesting something like it was done with the elasticsearch modules, whether you can opt which one you're going to install based on the configuration or to port the implementations to the contribs module and leave just the interfaces on the core and server modules? I'll add the documentation once I get the changes done.""]",0.9689996124886158,0.007737342473619927,0,0,0.00017065083321969653,0.0228782294418364,0
[],0,0,0,0,0,0,0
"['Hello @Ismaley, may you update this PR to resolve conflicts?']",0.8199261366804483,0.16511351036481056,0,0.0046981615798965355,0,0.006877492195180464,0
"['@Ismaley Hello, do you have any news on this feature?']",0.86079780459663,0.054385266402111915,0.07511474509525756,0,0,0.004320333416322657,0
"[""@andrea11 just updating here, I'm close to finish the changes, right now I'm trying to write an integration test case for this feature. Plus, I tried to run conductor on my machine to test this feature and I created a simple workflow with a http task, turns out that the task remains scheduled forever and the workflow never completes. Last version of conductor I tested was 2.11, did anything changed regarding creating workflow or task definitions? Thoughts, @kishorebanala? Tested it both with embedded and external dynomite.""]",0.9924724916763558,0.006677948161963579,0.00037211634396274476,0,0,0.00017527243905311618,0
"['@Ismaley Not related to the way tasks are executed. Do you by any change have system tasks workers disabled? Or, do you see any errors in your logs when polling for system tasks? [SystemTaskWorkerCoordinator](https://github.com/Netflix/conductor/blob/master/core/src/main/java/com/netflix/conductor/core/execution/tasks/SystemTaskWorkerCoordinator.java) is the entry point that polls and executed system tasks.']",0.7586977896355522,0.1789788386432524,0,0.059696869329528876,0,0.0016758898517413652,0
"['Hi @Ismaley, \\r\\nSorry I was busy recently. I read that you would like to improve the documentation later. But maybe it could be useful even for reviewer to better understand how this functionality works ðŸ˜„ \\r\\nAnyway, thanks a lot for your efforts to complete this PR. Could you try to fix your commit with the comments provided by @apanicker-nflx ?\\r\\n\\r\\n\\r\\n']",0.23953100818756426,0.760426894582529,7.690451988777848e-06,0,1.09137401320252e-05,0,0
"[""> I do not see a version 0.0.3 in NPM. where are you getting this version?\\r\\n\\r\\n@mstier-nflx so I think what's going on here is that if you go here directly to the url, you get the download you want:\\r\\nhttps://registry.npmjs.org/lazy-debug/-/lazy-debug-0.0.3.tgz\\r\\n\\r\\nBut really, I think that's the code from `lazy-debug`, not `lazy-debug-legacy`. \\r\\n\\r\\nThis this fixes the build for right now, so I think we should get this merged in so that people are not blocked from building/using Conductor. \\r\\n\\r\\nIt seems like some larger investigation should go into why we are even depending on this package really, or at least investigate upgrading `debug-fabulous`, which is what depends on this.""]",0.00018774009970037273,5.270301428436066e-06,0,0.5892068727008284,0,0,0
"['> I see that we have taken an approach in which we have defined the task that DO_WHILE needs to loop over before the actual DO_WHILE task which is similar to a JOIN construct.\\r\\n> \\r\\n> Can you please help me understand why this was chosen over a FORK like semantics in which the loop tasks could have been defined inside the task?\\r\\n> \\r\\n> I believe defining the do-while like fork would make more sense because loop-body should be inside the loop def.\\r\\n> \\r\\n> Thoughts?\\r\\n\\r\\nHi, I tried returning loopOver tasks in getMappedTask for do while task but the issue is in second time iteration. how we are going to iterate again. Also as far as this PR is concern my worry is I am calling scheduleTask from execute method, that two are different concern. Also if we are making do while like fork then we also need a way to handle input output for each loopOver task. Let me try once again.']",0.005238474232778987,0.8670296017941154,0,0,0.06807376379797349,0.05872065148298147,0
"['Hello,\\r\\nNice feature ðŸ˜„ Could you please provide more info on how this task should be used? (maybe updating the documentation)']",0.00010767598275680481,0.9998746577259993,0,0,0,1.1836767274942236e-05,0
[],0,0,0,0,0,0,0
"['Hello @manan164,\\r\\nany news on this PR? I was also thinking, that it could be useful to have a parameter to set a delay between loop. What do you think?']",0.0004912951945148675,0.9994888009068031,0,0,0,1.2956914164274467e-05,0
['Hi @apanicker-nflx\xa0 Any update on this thread?'],0.5939590924026553,0.025202570139566866,0.3642938342628814,0.011562791102824706,0,0,0
"['Hi, @apanicker-nflx Please let me know the updates on this. Should I write more documentation on how to use this?']",0.7166636183519354,0.16480040565681425,0.11664609001757018,0,0,0.0007203216569150369,0
"['Hello @kishorebanala @apanicker-nflx, do you plan to review this PR soon?']",0.9987534035476998,0.00097833080372877,4.217862423373446e-05,0,0,0,0
"['> Hello @kishorebanala @apanicker-nflx, do you plan to review this PR soon?\\r\\n\\r\\nHi, @andrea11\\r\\n Did you find the documentation useful? Are you using this?']",0.6506284090320765,0.34040833387112096,0.008453235597144936,0.0003213894276118932,0,0,0
"['Hi @manan164, I did not use this task, but I am very interested in it. I looked at the documentation and it seems clear :) \\r\\nIs there any limitation to the tasks that can be used inside this task?']",0.42869155699877787,0.2929126902108814,0,0.012293647665122935,0.25751229820484606,0,0
[],0,0.08609734536684634,0,0,0,0,0
"['Thanks @manan164 for PR.\\r\\n\\r\\n@apanicker-nflx Could you please share in which version, you are planning to release this change?']",0.46999115669387226,0.005104044573118593,0.1343261205931553,0.3885187297157364,0,0,0
['Do we need integration test with happy and sad cases?'],0,0.9998747769110656,0,0,0,2.4448321370587237e-05,0
['@apanicker-nflx\xa0 I have updated the PR with the changes as per PR comments. Can you pls take another look?'],0.8946457760490641,0.09414275297713298,0.005967216548997865,0,0,0.004653011195688087,0
['@kishorebanala @apanicker-nflx Can you please review this pull request?'],0.9999386567915716,2.6198589094558208e-05,0,0,0,9.044642178891307e-06,0
['@kishorebanala I will raise another PR for ES 5 eventually. Stumped right now. Could you please merge the current one?'],0.9960318390408357,0.0016024108540736964,0.002015962762350311,0.00015198678221667222,0,0,0
"['Hey @bharadwajrembar, are you still planning to work on this feature? We still feel this would be an important addition to ES modules.']",0.9508407022384644,0.005105764926002914,0.00044459580141046023,0,0.04351669158018778,0,0
[],0,0,0,0,0,0,0
"[""Thanks for submitting this!\\r\\n\\r\\n> Existing users with imports on conductor/* will still need to install the source code to the $GOPATH, but until they migrate their imports, they won't be able to update to using the go modules feature.\\r\\n\\r\\nCan you expand on this? I _think_ I get you, but I'd like to make sure.\\r\\n\\r\\nAlso, how does this PR affect the instructions provided in the [README](https://github.com/Netflix/conductor/blob/master/client/go/README.md)?""]",0.00941609330383378,0.9807362851824399,0,0.0010783394148241598,0,0.008554549776210294,0
"['> Can you expand on this? I _think_ I get you, but I\'d like to make sure.Absolutely - if a user of this library has already run the `install.sh` script, and has the following in their own code:```goimport""conductor""...conductorClient:=conductor.NewConductorHttpClient(""http://localhost:8080"")```When they upgrade to this new version of the library, they need to revise this import to:```goimport ""github.com/netflix/conductor/client/go""```Additionally, if they are not using go modules, the source code will need to be moved to `$GOPATH/github.com/netflix/conductor/client/go`> Also, how does this PR affect the instructions provided in the [README](https://github.com/Netflix/conductor/blob/master/client/go/README.md)? Ah, good point.  These instructions would need revision; should I add this to the PR?']",0.004733448369134873,0.08735809268867821,0.008125674815498063,0,0,0.8953592854538278,0
"[""@jvemugunta @mashurex Could y'all take a look at this? Or if you are no longer able to contribute, can you at least respond here so that we're aware if you're looking at this? Thanks.""]",0.9999766811478886,9.944036830216511e-06,0,0,0,0,0
"[""@apanicker-nflx @s50600822 I reviewed this and everything is working against MySQL 5.7 as well as the tests passing. One other thought, if we're breaking the migration hash chain for this, couldn't we just change the V1 migration to include this column (as well as the V2 changes)? It would seem cleaner going forward.""]",0.007132320797308739,0.9920554609939922,0,0,0.00021126949752704304,0.0004315865017399382,0
['I having this issue with latest released jar : https://repo1.maven.org/maven2/com/netflix/conductor/conductor-server/2.22.2/\\r\\n\\r\\nIs there any work-around?\\r\\n'],0.003063183635820313,0.16543797960315024,0,0.00010465220908772836,0.8311087884495516,0,0
['Can this fix get merged?'],0.00024229004897770182,0.999433545895738,0,0,0,0.00024531928298134406,0
"[""@apanicker-nflx Would you mind merging this PR if that's fine by you?""]",0.9971343329849499,0.002582013908129405,0,5.466390163144414e-05,0,0,0
"['Hi @s50600822. There are some failing tests in `mysql-persistence` because the thrown exception of `withTransaction()` has changed from `ApplicationException` to `RuntimeException`. As far as I understand, some code can be broken because of that as there are a couple of `catch (ApplicationException e)` in the codebase.\\r\\n\\r\\nA quick fix that I see here is to update the method `retryOnException()` to re-throw the original exception depending on the configuration (or even to create a new method). what do you think?']",4.729665863130456e-06,1.4951920362856293e-07,0,0,0,0,0
['@ggrekhov what about the test itself ? I think if it perform insert twice and end up getting an Exception does it need to be ApplicationException?'],0.05401471335467097,0.945844533826124,0,2.5554886682839974e-05,0,2.5165891303109074e-05,0
[],0,0,0,0,0,0,0
['Are you going to merge it anytime soon?'],0.004186161812782977,0.9917636461649816,0.00036584127261853267,0,0,0.00317849170917267,0
['ping?'],0.06675005217861409,0.4926087946119677,0,0.4393604641412611,0,0,0
['@goldibex Are you available to continue working on this PR?'],0.9998291323495401,7.593122967649943e-05,0,0,0,0,0
"['Hey @choubisat, Are you still considering working on this feature?']",0.9916805130782619,0,0,0.00020116039159060947,0.006629196988453804,0.0010834238727008448,0
"['Hey guys, this system task will help simplify our workflows very much. Are you considering continuing to develop it? (@choubisat , @kishorebanala , @apanicker-nflx )\\r\\n\\r\\nBy the way, suggesting to limit this to strings only (which in the worst case can be serialized/deserialized to/from JSON) and the max length can be set in the config with default of 512 characters.']",0.996844497496194,0.0031364678523805294,0,0,0,0,0
"['@elisherer Since @choubisat does not seem to be active any longer, would you want to take up and continue this development? Please let us know, if you have a few cycles to spare for this.']",0.9987503154069504,1.1378829391259819e-05,0,0,0.0012125499986534154,0,0
"['Hello @apanicker-nflx, Could you kindly please review this change? Thanks!']",0.9399268480215393,0.05964720429615657,0,0,0,0.00021474591916597962,0
"['Hi @apanicker-nflx, what we can do here is have all indexing workflow operations should be sync. Also, we should minimize the number of indexing workflow calls. I think we have discussed this in other [thread](https://github.com/Netflix/conductor/issues/1243#issuecomment-522809341) But still, I feel the same problem will be there with the task also. \\r\\nCan you please elaborate on how you are trying to solve this by ScheduledThreadPoolExecutor. By using that we only ensure that tasks will be scheduled after a particular interval. Here what we lack is ordering guarantee. \\r\\nhow we are trying to solve this is, publish all index operations to Kafka which will give ordering guarantee. And consume from Kafka. I have raised a sample [kafka-persistance](https://github.com/Netflix/conductor/pull/1337)\xa0 also. The only thing I want to know in that is how should we write consumers. Please go through that once. Again this is a niche optimisation and not all people will be interested for this. For those who are not interested we can use sync indexing only in case of a workflow. So basically I would say the flag you have used for this will separate out high throughput and low throughput scenario.\\r\\nViews?']",0.008662122448540676,0.8301471705450898,0,0,0.0012563419434740446,0.15971493644310417,0
"['Thanks for your suggestions and pointers.\\r\\n\\r\\n> what we can do here is have all indexing workflow operations should be sync. Also, we should minimize the number of indexing workflow calls. I think we have discussed this in other [thread](https://github.com/Netflix/conductor/issues/1243#issuecomment-522809341) But still, I feel the same problem will be there with the task also.\\r\\n\\r\\nIn-sync indexing would not be ideal since this keeps elasticsearch in the request path, but users can opt to do that if the throughputs in their use-case is quite low. This can be controlled via configuration. \\r\\n\\r\\n> Can you please elaborate on how you are trying to solve this by ScheduledThreadPoolExecutor. By using that we only ensure that tasks will be scheduled after a particular interval. Here what we lack is ordering guarantee.\\r\\n\\r\\nThis is correct, by delaying the update operation by a fixed interval. This does not guarantee ordering but greatly reduces the probability of this inconsistency.\\r\\n\\r\\n> how we are trying to solve this is, publish all index operations to Kafka which will give ordering guarantee. And consume from Kafka. I have raised a sample [kafka-persistance](https://github.com/Netflix/conductor/pull/1337) also. The only thing I want to know in that is how should we write consumers. Please go through that once. Again this is a niche optimisation and not all people will be interested for this. For those who are not interested we can use sync indexing only in case of a workflow. So basically I would say the flag you have used for this will separate out high throughput and low throughput scenario.\\r\\n> Views?\\r\\n\\r\\nWe will be reviewing the Kafka-persistence PR soon. Eventually, in the long term, we would also be looking to move to Kafka as an intermediate layer for indexing. \\r\\n\\r\\n> The only thing I want to know in that is how should we write consumers.\\r\\n\\r\\nHave you considered writing a Flink job to consume from the Kafka topic? This job could handle de-duping and merging before indexing the document to elasticsearch.\\r\\n']",0.0001414926196771362,0.9996176630394112,0,7.293682466836228e-06,0,0.00021732187684468074,0
[],0,0,0,0,0,0,0
"['Hey @ggrekhov, Are you still considering working on this feature?']",0.996270430784162,0.0001374387767521052,0,0,0.0026990376376724084,0.0005657060279136138,0
"[""Hi @kishorebanala. To be honest I didn't get why my solution is wrong.\\r\\n\\r\\n@mashurex, you say: `this is going to lead to data consistency issues and (probably) the same task being handled twice`. I'm looking at the code now and I cannot notice a way how a message can be handled twice when actually it can be popped only by 1 worker. What consistency issues do you mean?\\r\\n\\r\\nI admit that my implementation brings a bit new behavior of `pop()` - sometimes it can return less messages than a client asks for. I'm not sure how critical it is for the whole design - you know it much better than me. But it seems not critical to me - with my knowledge of the system I couldn't notice any bad impact of it, and I see the positive impact of the change in general as the messages don't get stuck in the queue forever.\\r\\n\\r\\nBy the way, this PR has been deployed in our production instance for several months already - we didn't experience any inconsistency issues in our workflows.""]",0.00032221549407526545,0.00010465241839235586,0,0,0.9995011844998384,5.060900667403285e-05,0
"[""@apanicker-nflx @kishorebanala @ggrekhov @mashurex Could y'all follow up with @ggrekhov 's comment? It looks like the Postgres one was approved and merged: https://github.com/Netflix/conductor/pull/1741 I'm not sure what the difference would be between Postgres and Mysql implementation where one would be ok, but other not in this instance.""]",0.9902630560174982,0.00969594618400596,7.121394464223858e-06,2.2422536749481853e-05,0,0,0
"[""> @checkingchecks I'll look into this\\r\\n\\r\\nhi, any updates? so I initially created the same schema and tables via script to set it all up before running the server but when I ran the server it still prompts the flyway migration exception even though it has the similar tables and indexes before running.""]",0.0009684477751591525,0.9988650707199379,0,0,0,0.00011015710322048099,0
"[""> @checkingchecks While flyaway has an option for baselineOnMigrate, conductor does not. I created a branch that adds this configuration option to conductor. To test this, create a new build from my branch:\\r\\n> \\r\\n> ```\\r\\n> git clone -b baseline-on-migrate https://github.com/mactaggart/conductor.git\\r\\n> cd conductor\\r\\n> ./gradlew build -x test\\r\\n> ```\\r\\n> \\r\\n> In your conductor config, set:\\r\\n> \\r\\n> `baseline.on.migrate=true`\\r\\n> \\r\\n> If this works in your environment, I can submit a PR. Hope this helps.\\r\\n\\r\\nThanks. Pls bear with me as i'm new to this. I'm running `java -jar <server>.jar <server>.properties`. When applying `baseline.on.migrate` on the properties file, it seems the change is not reflected. I think it's because this change you committed is only applicable to the UI not the server and my properties file is accessing the fields available to the server. Am I doing things correctly?""]",0.00024808726467569583,0.999364370498076,0,0.00017524299575666166,0,0.00015858386307488536,0
"['Hi @shivambansal2222 , I have an overall look over the PR.\\r\\nPlease answer the following questions,\\r\\n1) We are changing the way task keys are generated. [Key Definition](https://github.com/Netflix/conductor/blob/master/core/src/main/java/com/netflix/conductor/dao/ExecutionDAO.java#L62) says we should use only ref name and retryCount. So if we are changing the way keys are getting generate then we should update the method documentation in the interface. Another thing is, currently we have CassandraExecutionDAO and MySQLExecutionDAO. We might need to change there. Also in the future, if we are going to have another implementation we might need to change there. I was thinking to change in case of do while but later dropped the idea considering future maintenance.\\r\\n2) Instead of changing all task mappers, we can change the way tasks are getting scheduled. Please explore how we are doing this in case of [do-while](https://github.com/Netflix/conductor/pull/1169#discussion_r327177275) The changes will be very minimal because we already have changed the way task keys are getting populated. In [GoToTaskMapper](https://github.com/Netflix/conductor/pull/1373/files#diff-02cefa0c6fb9c2346a4e8459eb37329cR101) update the iteration of the gototask and when it gets completed in [DeciderService](https://github.com/Netflix/conductor/blob/master/core/src/main/java/com/netflix/conductor/core/execution/DeciderService.java#L192) automatically it will update the iterationCount of all subsequent tasks.\\r\\n3) How are we handling the case of gotoTask will be subworkflow?\\r\\n']",0.002826909255521667,0.9962487784764197,0,3.801305137382089e-05,0,0.0008511331318242497,0
"['Hi @shivambansal2222,\\r\\nI would like to understand if in the workflow execution we will find all the execution of a same task (in your example LeReview), or only the last one?\\r\\nIf all of them are stored, will they have the same taskReferenceName (and different retryCount) ?\\r\\nIf yes, how can you distinguish between a fail of the task and an execution demanded by a GOTO task?\\r\\nIf no, how can you select the last execution via taskReferenceName as variable inside the workflow?']",0.0018325332129038314,0.9736017537558463,0.0006417026009487189,0,0,0.023684380470235786,0
[],0,0,0,0,0,0,0
"['Hi @manan164, Is there a roadmap as to which release will this PR be merged to?']",0,0.0009518517120108255,0.0004219541074333195,0.00038703400522356005,0,0,0
"['Hi @shivambansal2222, we would also like to use GOTO functionality. Is this work still relevant?']",0.02012024614456768,0.9749309802277358,0,0.0009566490047336785,0,0.002502867260750689,0
"[""I've optimized the test case. Do you have time to review it? @apanicker-nflx""]",0.9692286790908472,0,0.019016012339947434,0.0007201626994669555,0,0.010118225260678685,0
['Does this *require* an ownerEmail now on a task?'],0.00470273699687258,0.9951013678266117,0,0,5.607440995716931e-05,0,0
[],0,0,0,0,0,0,0
['@manan164 Is the work for this PR still active?'],0.9982753780724708,6.684052497926815e-05,0,0.0015842513779189052,0,0,0
"['@mudit3774 this is important for us. @kishorebanala , when this is supposed to go, live?']",0.20023698277810176,0,0.5467394311104125,0.2512971941085747,0,0,0
[],0,0,0,0,0,0,0
"['Hey @DemisB, are you still planning to work on this feature?']",0.8883856586406774,0.031816466778401865,0.07693041246216954,0,0,0.0012329075960309734,0
['@apanicker-nflx and @mstier-nflx I was wondering if we could get this merged soon?'],0.870144655904871,0.0687025130750519,0.04579709123645137,0,0,0.012510083248113488,0
['So how to create a new http task with new configuration? Can you give me an example? Since with previous configuration my http tasks are always in running state'],0,0.0002147753413722802,0,0,0,0.999741197265207,0
"['@oanaema Please provide a description as to why do you think these fields need to be part of the copy method in the Task object? Currently, the main purpose of the `copy` method is to help copy over required fields during retry operation during which the startTime, scheduledTime, endTime should not be copied over.']",0.0005862585817097794,0.3845275427354424,0,0,0.6145975104360659,0,0
"[""Hello @apanicker-nflx\xa0 ,\\r\\n\\r\\nThis PR was created to fix the issue we have on the Task level with some of the fields that are missing from copy method. We should be able to get scheduledTime, startTime and endTime parameters from WorkflowTasks in a workflow. Instead of getting such dates, we are always getting 0 as a value, because in the copy method multiple fields are reset.\\r\\n\\r\\nIs the copy method only used during the retry operation? I've seen that on a usual task execution on **populateWorkflowAndTaskData** from **DeciderService** the _copy task method_ is triggered by _the copy workflow method_ and the startTime, scheduledTime, endTime are reset.\\r\\n\\r\\nCan you please advice what we should do in this case, since startTime, scheduledTime, endTime are not available after task execution?\\r\\n""]",0.09685681742709,0.8824032128620549,0,0,0.012891681765280988,0.005719928349386026,0
"['Hello,\\r\\nThis PR was created to fix the issue we have on the Task level with some of the fields that are missing from copy method.\xa0 We should be able to get scheduledTime, startTime and endTime parameters from WorkflowTasks in a workflow. Instead of getting such dates, we are always getting 0 as a value, because in the copy method multiple fields are reset.\\r\\nFor instance I saw that seq and startTime fields need to be reset because they are taken into account in some business logic.\\r\\nCan you please advice what we should do in this case, since we cannot touch startTime because we are breaking other logic?']",0.00024402228618945472,8.123472015976984e-07,0,0,0,9.811171162011818e-07,0
"['Thanks for the context. This is a valid bug and is indeed an edge-case. Looking at the linked code, I feel that it would be easier to move```executionDAOFacade.updateWorkflow(workflow);```\xa0 after line 721. This would prevent having to add a dependency to IndexDAO in the WorkflowExecutor. Would need to ensure that the workflow object is updated with the updated tasks (new status). Also, we are looking into indexing task documents during updateTask to enable task level searching for in progress workflows. Once this is implemented, it would solve for this issue as well. Thoughts?']",0.00021314880950793838,0.9997272578209124,1.713867418306021e-05,0,0,1.76855258767198e-05,0
"[""To me it looks like either of your solutions is better than doing a one-off index of the task when needed as I did but I admit that I don't know the reasoning behind not putting the indexDAO.indexTask() call in updateTask() in the first place. Can you explain? I'm sure there is a reason and I don't want to start changing code that is integral to the functioning of Conductor unless I understand all the implications.\\r\\n\\r\\nThe problem with not calling indexDAO.indexTask() during the updateTask() method from my perspective is the /tasks/search endpoint. If you use that endpoint and your freeText is, for example, status:CANCELED then you get inaccurate results because that endpoint does not find the CANCELED tasks as ElasticSearch is used to do the initial query. I think the same issue surfaces for other statuses but I haven't yet looked into it further.""]",0.005855493491510451,0.004930489528575234,0,0,0.4334810923394808,0.5555083158430558,0
"[""I think the reason that updateWorkflow() is called before the updating of the tasks (during terminateWorkflow()) is because we don't want to have a task update exception thwart the updating of the workflow. Please correct me if you disagree. I therefore like your second solution of adding a call to indexDAO.indexTask() in the updateTask() method. That will create a ton more volume for ElasticSearch but I would rather have both ElasticSearch and the database in sync with more volume than worry about volume.\\r\\n\\r\\nThe question is: are there situations where adding something to ElasticSearch (where it currently is not added) would cause a problem for another use case? I don't know the answer to that...""]",1.4485375553740193e-05,9.54148904788561e-06,0,0,0.9999607083559369,0,0
"['> Are there situations where adding something to ElasticSearch (where it currently is not added) would cause a problem for another use case?\\r\\n\\r\\nYour observation is correct that this will create a large volume of requests to ES and in case of synchronous indexing, this can introduce a latency cost. However, I agree that it is better to have both Elasticsearch and primary datastore in sync in such situations. \\r\\n\\r\\n>\xa0 I therefore like your second solution of adding a call to indexDAO.indexTask() in the updateTask() method.\\r\\n\\r\\nThis change can be added such that it is performed only in cases where async indexing is not being used. Otherwise with the current in-memory queue async indexing model, there is a high possibility of indexing requests being dropped.MESSAGE 213 > This change can be added such that it is performed only in cases where async indexing is not being used. Otherwise with the current in-memory queue async indexing model, there is a high possibility of indexing requests being dropped. So, if I add this code to ExecutionDAOFacade.updateTask(Task task): if(!config.enableAsyncIndexing()) {indexDAO.indexTask(task);}`then I would need to bracket every existing call to indexDAO.indexTask() with this:if(config.enableAsyncIndexing()) {indexDAO.indexTask(task);}`Is that acceptable to you? It could get messy I am thinking.']",0.01120758273333829,0.13828679855518294,0,0,0.00532309701144925,0.8436506600195351,0
"['> This change can be added such that it is performed only in cases where async indexing is not being used. Otherwise with the current in-memory queue async indexing model, there is a high possibility of indexing requests being dropped. So, if I add this code to ExecutionDAOFacade.updateTask(Task task):` if(!config.enableAsyncIndexing()) {indexDAO.indexTask(task);}`then I would need to bracket every existing call to indexDAO.indexTask() with this:`if(config.enableAsyncIndexing()) {indexDAO.indexTask(task);}`Is that acceptable to you? It could get messy I am thinking.']",0.00032949117084344774,0.06786426986015155,0,0,0.018402569083692755,0.9133777664990157,0
"[""@rickfish, @apanicker-nflx - Is this issue fixed? I don't see relevant code changes in the new pull request is #1526.""]",0.8920683355281339,0.10538067593066885,0,0.001561999262056147,0.0004296386720855318,0,0
['@mactaggart @jvemugunta @gorzell Can you help review this please?'],0.9995365489766181,0.00044084042326873204,0,0,0,1.0826377937812591e-05,0
"[""A quick question as a new user to the current state of things with the /ack endpoint.\\r\\nI'm seeing the following odd behaviors right now:\\r\\n- /ack endpoint always returns false\\r\\n- /poll endpoint returns the same task more than once when called concurrently from different worker threads (from C# client for example)\\r\\n- Python client calls ack and only proceeds if ack returns true, so it never perform any actions\\r\\n- C# client runs the ack endpoint but always proceeds regardless of the ack result.\\r\\n\\r\\nSo in other words - right now the C# client ends up processing the task more than once and the python client ends up not processing anything at all.\\r\\n\\r\\nSince the /ack endpoint is deprecated, shouldn't the /poll endpoint always only return the task exactly once?\\r\\nAnd shouldn't /ack always return true for backwards compatibility with clients like the python client that will only proceed if it returns true?\\r\\n""]",0.0017645633728889788,0.9976524448277214,0,0.00018538515095062544,0,0.0002131281350761422,0
['when will this feature be released?ðŸ˜„'],0,0,2.4453064805031405e-05,0,0,0,0
['@shashi1996 Are you actively working on this PR?’'],2.17871328209304e-05,1.9743938995898418e-05,0,3.730455950668255e-05,0,0,0
"[""How do I know what the failure was? There doesn't seem to be a way for me to see what the errors were. It built fine for me.""]",9.428750243059697e-06,0.9999217310531738,0,0,0,6.0398849573575796e-05,0
['@ritu-p Have you had a chance to take a look at the comment here about basicConsume? https://github.com/Netflix/conductor/pull/1063#issuecomment-477244947'],0.7234560363466032,0.005463217921059382,0.00016729932243160324,0.27083846660703403,0,0,0
[],0,0,0,0,0,0,0
"['@ritu-p Looks good to me. Please address the above comment on `setUnackTimeout`, we should be good to go. Thanks for your contribution ðŸ‘\x8d \\r\\n\\r\\n@jkaipa Do you have anything else to add?']",0.28499125026765837,0.7147972092095665,0,0,0,0,0
"['Hi @ritu-p @mekahell \\r\\nI have been using this ampq feature and i found an issue declaring an amqp queue in an event handler.\\r\\nAs part of my workflow definition, I created an event-handler to complete a WAIT task and set the event queue to be ""amqp:some_queue_name"". After starting a workflow i noticed that after a message was sent to this queue, the message was consumed but the event-handler action was not executed.\\r\\nAfter debugging, i found out that after the message is received, conductor is not able to find the event-handler because internally it is stored as ""amqp_queue:some_queue_name"".Since the queue type strings are not the same no handler is found and no action is executed. (BTW if i use amqp_exchange it works).\\r\\nI took a look at the code changes in the PR and i saw that the StringMapkey ""amqp_queue""\xa0 was changed to ""amqp"". \\r\\nIs there any workaround for this issue? is it possible to rename the StringMapKey back to ""amqp_queue"".\\r\\nThanks']",0.00014006499403154286,0.9995385745095009,0,1.437383525935173e-05,0.00028734010757485795,0,0
"['Actually, after looking at it further - maybe PR #1658 is a better, cleaner solution than mine. I am thinking about closing this PR. Thoughts?']",0.1027481942468628,0.10325741848380296,0,0,0.008530329413452541,0.7842343872368865,0
"[""Sorry for the multiple comments. I just tested the PR #1658 change and it doesn't handle terminating the subworkflow if there is one. So I am stuck trying to figure out what that PR does that mine does not. \\r\\n\\r\\n@kishorebanala can you elaborate? do we need both PR's? I don't know what mine is missing that causes yours to be needed?""]",0.0011359019204983073,1.5450797976251638e-06,0,0,0,0,0
"[""@kishorebanala Your use case is opposite of mine. If the parent workflow has a terminate task and it has a sub-workflow, that sub-workflow is not handled properly, i.e. tasks in it are still SCHEDULED. I am going to attach a sub-workflow (SIO_ETD_TERMINATE_SUBWF) and a parent workflow (SIO_ETD_TERMINATE_WF). If you start the parent workflow, then complete the 'basicJava0' task you will see the results. My change takes care of handling the sub-workflow, not sure if it is the best solution but it works. LMK if you see differently and we will compare notes. Actually, what is the best way to upload JSON files? I tried to attach and I guess you can only attach images? Just tried a .tar file as well as .zip as well as multiple .txt files and got this error in all cases:Something went really wrong, and we can't process that file. Try again.Can I email them to you? Basically, the sub-workflow just contains one WAIT task.""]",0.038685957503538655,0.4650550933920592,0,0.012834402732795817,0,0.4794113412912502,0
"[""Is this ever going to be merged? This is the second time, while waiting for it to be merged, some other change came in and made me do everything all over because it changed the underlying code. If it is going to be merged i will make my changes again. If not, please let me know so i don't have to do it again. I do believe this is a necessary change.""]",0.006478887276191876,0.019969970675163165,0.0035028197219271213,0,0.9698296611182765,0,0
"['Hi @kensou97 ,\\r\\nReally nice work. Even we are facing this issue in our environment.\\r\\nHave you tried considering [WorkflowSweeper](https://github.com/Netflix/conductor/blob/master/core/src/main/java/com/netflix/conductor/core/execution/WorkflowSweeper.java)?\\r\\nIt is not helpful for use because while doing redis.pop we are facing [issue](https://github.com/Netflix/conductor/issues/1595).\\r\\nAlso, I feel in case of the environment having short-lived workflows ~ seconds and long-lived workflows ~days, this approach will clean both together. Instead, I feel we should add the variable **workflow.cleaner.expire.seconds** in the workflow definition \xa0 \\r\\n@mudit3774 . This is important for us.']",0.229464969379802,0,0,0.01709856050518968,0.6654165656038727,0.0842226155978516,0
[],0,0,0,0,0,0,0
[],0,0,0,0,0,0,0
[],0,0.12845212656210414,0,0,0,0,0
['This PR seems to undo the work done in #1280. @s50600822 Can you please comment on these changes?'],5.848489746779081e-05,0.0009058199521333697,0,8.59731946854644e-05,0,0,0
"[""> This PR seems to undo the work done in #1280. @s50600822 Can you please comment on these changes?\\r\\n\\r\\nOne small difference, i just fixed it by removing 'IF NOT EXISTS' syntax. Mysql DDL can't support 'IF NOT EXISTS' syntax, but it works fine for miradb, and the syntax for #1280 is also for miradb.""]",0.14717362010364915,0.8519189038139506,0,0,0.00014908871895325182,0.0006446637467324209,0
"[""> > This PR seems to undo the work done in #1280. @s50600822 Can you please comment on these changes?\\r\\n> \\r\\n> One small difference, i just fixed it by removing 'IF NOT EXISTS' syntax. Mysql DDL can't support 'IF NOT EXISTS' syntax, but it works fine for miradb, and the syntax for #1280 is also for miradb.\\r\\n\\r\\noh, why part of my syntax is for miradb only ? Also migration script need to be idempotent so you can't just remove `if exists`.""]",0.001678754717507039,0.011767606904563131,0,0,0.9863676980319205,6.348493019463177e-05,0
"['Hey @kishorebanala, when do you think we can get this in ?']",8.769777758616387e-05,0,0.9994261631074028,0,0,0,0
"['Hey @apanicker-nflx, @kishorebanala \\r\\n\\r\\nIs there anything else I can do to get this PR merged ?\\r\\n\\r\\nBR, Maros']",0.9720620977386115,0.027534706926390606,0,0,5.35351023779715e-05,0.00025869649949507474,0
[],0,0.09308855821962979,0,0,0,0,0
['@manan164 @rickfish Are we good to merge here?'],0.9499416075628132,0.027341770259053835,0,0.005930544197185951,0,0.015095871245495322,0
"[""This would be a difficult test case to add as it involves having one of the loopover SIMPLE tasks time out and then polling and completing that task only after it has timed out the first time in the first iteration. Do you think it is worth that? Since it is a bug fix, wouldn't the fact that it doesn't break the existing do-while test cases be enough?""]",0.0026225976052183563,0.669559999161522,0,0,0.2830162676864483,0.043269780394288304,0
"[""@apanicker-nflx thank you. If I wanted to attach the workflow/subworkflow metadata to this PR so you know the specific scenario I use to test this issue, how would I do that? I have tried to use the 'Attach files by dragging : dropping, selecting or pasting them' link but it always gives me an error.""]",0.0015418550647346065,0.00034417872021293557,0,0.9483919431778991,0,0.049549981082117854,0
"['So that issue ( #1696 )is all about the UI, right? The DO_WHILE task is working as expected - just not being displayed on the UI, right?']",0.00012065027338032839,0.9997762237016518,0,2.3179159046075825e-05,7.263795330031299e-05,0,0
['@apanicker-nflx - Is this issue fixed and merged to master? I am also seeing this issue.\xa0 I merged the changes from Netflix master on 01-Sep-2020.\\r\\n'],0.10776269908559356,0.1975228510134341,0.6394908667027129,0.044760698061742654,0,0,0
['@tjasko Have you added these changes in your code base?'],0.9636144138692359,0.03617227635889409,0,7.396888693477646e-05,0,0,0
"['I tried reproducing the issue with the DECISION task in our test environment and am unable to do so. Can you provide some details about your setup - persistence, queue, lock implementation etc. ? Thank you.']",0.0017799838282005578,0.9943750893844581,0.0015802903231403535,0,0,0.0011942976583262143,0
"[""Hey @mdepak, Just to confirm, are the `workflow_execution` and `workflow_completed `'s count metrics being compared at same cadence? Say per second?""]",0,0.040232795801355525,0.6869635946086026,0.2604487172324583,0,0.011345130022957596,0
['@apanicker-nflx Can you please review this? This PR is expected to solve problems related to a couple of use cases in archiving workflows. Thanks for your help!'],0.0990492279912206,0.8776417659963464,0,0,0.022292483948407955,0.0005586563680714029,0
"[""@kishorebanala\xa0 thx, copying my reply> Hey @kishorebanala , thx for the review. The reason for the above is that there is only a NoopLocking strategy utilized by default (in ExecutionLockService). Correct ?>> Which means that the solution I proposed still requires a real locking strategy. However it would be sufficient to have a localOnly locking strategy (something based on pure Java semaphore) that does not require persistence... As long as I use the localOnly distribution strategy with it.>> What I will do is:>> - Add a localOnly locking provider (that can handle locking inside a single node)> - Update the docs>> What do you think ? I didn't realize the default locking is implemented by NoopLock... UPDATE: I have added the LocalOnlyLocking provider... which should solve the race condition in a single node or in a cluster node with localOnly sharding for dyno queues. Also I think a LocalOnlyLocking provider could be used as the default instead of Noop ...""]",0.03021687910415906,0.6879428922118315,0,0,0.16041552483263763,0.12035163854668447,0
['@kishorebanala rebased and fixed the comments. \\r\\n\\r\\nOne additional question: Should we want make the LocalOnlyLock default ? Instead of Noop ?'],0.6620994683679035,0.06076204164652811,0,0,0.2765300179217738,0,0
[],0,0,0,0,0,0,0
"['Hi @kishorebanala, any update on the review?']",0.025448784434864022,0.0035656746513665813,0.9664715440819819,0.0032001359042505098,0,0,0
[],0,0,0,0,0,0,0
['@mactaggart Can you please help review this PR? Thanks\\r\\n'],0.9946176182035521,0.005297787491586794,0,0,0,0,0
"['@ritu-p \\r\\nHi, i opened this PR for the issue we talked about, when do you think it will be merged?\\r\\nThanks']",0,0,0.00012444533684016797,0,0,0,0
"['Hey @ritu-p , being an active contributor to AMQP module, does this PR looks good to you?']",1.4518701223291497e-05,6.491907913696808e-05,0,0,0,0,0
['@apanicker-nflx Added tests. Do you think it is better to move out `com.netflix.conductor.server.resources.HealthCheckResource` from `jersey` module into a dedicated module ?'],0.03026206896574695,0.9644088500260936,0,0.005104677513887174,0.0001331855885207598,0,0
"[""@apanicker-nflx Thank you for review. There is an issue where the execution history is not automatically cleared after executing the current eventExecution. Is there any way to improve it? I'm trying to develop eventExecution History Delete api, so please check if there's any other way.""]",0.005299406212912565,0.9944361488726702,0,5.496609595440292e-05,0.00012147148671104999,0,0
"['@kishorebanala Oh, nice. It is much cleaner.\\r\\n\\r\\nSo does this mean my PR can get merged?']",0.06169029831879209,0.9358726494885832,0,0,0,0.0017867416949626362,0
"[""@kishorebanala I have added a test case for this. *Please* merge this into the next release as I am having to constantly do special merges so that my code doesn't get lost. Unfortunately I don't know how to run these spock tests. How do I run them to test locally?""]",0.3202516455144727,0,1.0265795645253347e-05,0,0,0.6797181717706081,0
"['@apanicker-nflx , Hi, can this be merged into next version?']",0.5980534785599655,0.39434036975071435,0,0.004160104852910683,0,0.0016390474976305273,0
['@mactaggart @mashurex Can you please help with reviewing this PR? Thanks'],0.9990248128043906,0.0009265894229044364,0,0,0,0,0
['@ritu-p Could you please help with reviewing this PR? Thanks'],0.9699341591048166,0.02449220929400882,0,0.005256554000030573,0,0,0
['@apanicker-nflx Has the above been merged to main branch too ? How to figure out if this has been merged to main branch?'],0,0.07212482359356384,0.037032821203080456,0.030496144902403745,0,0.8326463056882382,0
['@alex-fu Thanks for the contribution. Can you please also add an integration test for this [here](https://github.com/Netflix/conductor/blob/master/test-harness/src/test/groovy/com/netflix/counductor/integration/test/DoWhileSpec.groovy). Could you also add an integration test which would handle a failure and automated retry in one of the forked branches within the do_while? @manan164 We would appreciate it if you could help with reviewing this PR.'],0.4068295000933693,0.5831566688511892,0.0001629575207528708,0,0,0.00950120829040238,0
"['Hi ,\\r\\n I have embedded the DO_WHILE in a decision task and I am facing a similar issue where the task outside the loop task is getting executed.Any timeline by which this PR will be merged?']",0.004430760673818211,0.7119652677571233,0.2792864838915117,0,0,0.0024413502782912678,0
['@alex-fu Are you available to continue working on this PR? Thanks'],0.9994924906764456,0.000314121407090373,7.235073109225246e-05,0,0,3.2126831651023144e-05,0
"['@mactaggart @rickfish Can you please help with reviewing this PR? Also, do you think a similar change would be needed in the mysql-persistence module?']",0.9987014401594435,0.001260110327933603,0,0,0,0,0
"[""@apanicker-nflx I don't see anything Postgres specific here, so we'll need to update the MySQL code too. As @rickfish mentioned, the Postgres module is based on the existing MySQL implementation, with minor pg-specific differences between them. I had originally submitted a PR that extracted common code between MySQL/Postgres into a sql-persistence module. Perhaps this approach could be revisited since it would cut down on duplicate logic?""]",0.00862027493330671,0.9913448657100972,0,4.861988730239063e-06,0,1.5142942138552427e-05,0
"[""Don't embedded task definition already accomplishes that?""]",0.0005391819901398238,0.9991533292281389,0,0,0,0.00023176978152206744,0
"[""@elisherer Looks good to me. Can you also add an integration test that verifies this task execution end-to-end please? We've been moving towards Spock based integration tests, and here's one example to get started: https://github.com/Netflix/conductor/blob/master/test-harness/src/test/groovy/com/netflix/counductor/integration/test/DecisionTaskSpec.groovy""]",0.0023674252660798897,0.9968632176141552,0,1.955213637518856e-05,0,0.0007318709919797213,0
[],0,0.0640580484290877,0,0,0,0,0
"[""> @heinrichcohn @elisherer Thank you for taking over this feature.> > I would love to know your thoughts about adding constraints to prevent misuse of this task. So we actually talked about it, seeing your comment on the last PR, and I was thinking that every user of this product will surely begin by going to the documentation. So that's why I added a warning to the docs: ```md !!!warning Use variable values with caution for payload size. Do not use large payload since these values are not externalized to the [external payload storage](../externalpayloadstorage).```I think no matter what the prevention measure will be, it might cause more bad behavior than good. Since if we put limits on the variables size (we need to check it on every step, because it gets changed), the limits can go over size of a single value. limit on the amount of variables. And considering edge cases; what happens when a limit is changed (for the worst) and there are workflows that go over the limit? If at all, we could plan to move the variables values to the external storage as well but that defeats the purpose. It suppose to be simple small values to help with the orchestration. for us it is useful in failure workflow to understand what have been done already to setup the cancellation.""]",0.016046428139478406,0.7325538686322689,0,0,0.058381419442664895,0.1913592552704035,0
"[""Hey @elisherer & @heinrichcohn, thanks for picking up this long awaited feature.\\r\\n\\r\\nWhile documentation definitely helps, we could still run into a situation where setVariables is called enough number of times, knowingly or accidentally to become a large payload blob. This could in turn cause unforeseen issues in Persistence layer, for eg., wide rows in Cassandra, which we'll have limited control over.\\r\\n\\r\\nHow about checking the variables hashmap size in SetVariable's `execute` method, everytime this task is executed? I.e We shall limit max size to a configurable value defaulting to 256KB, and reject any updates to this hashmap over this limit by failing the task.""]",0,0,0,0,0.0017126395535911497,0.9982665866992374,0
"['OK, should we add that check to the LAMBDA task as well? (Since it will be possible to set variables from there too)']",0.0013305171203808067,0.9955456491793372,0,0.00014418952181566915,0,0.002796939486894803,0
"['Hi @mdepak , \\r\\nThanks for pointing out. \\r\\nAre you using clustered redis?\\r\\nBut for the happy case also we see that the message is not removed from .MESSAGE.HTTP even after successful execution. Messages get piled up in high throughput scenario. This is the same behaviour for all other queues also.']",0.0013060661481301594,0.998610492660511,0,0,3.1565146655897665e-05,2.5354609757030124e-05,0
"[""@manan164 We use the Redis cluster for our deployment. `HTTP` is an async system task and they are polled also polled and executed using SystemTaskExecutor and hence it is applicable for HTTP system task as well leading to a pile-up of messages in `*.MESSAGE.HTTP` hash. When the task is polled by the task workers' clients, it is acknowledged by the client and thus corresponding message payloads are removed. In the deprecated `WorkflowTaskCoordinator` the tasks are acknowledged once the task is received. However, in the `TaskRunnerConfigurer` once the task is polled it is not acknowledged and that could lead to a similar problem related to non-system tasks with RedisDynoQueues. @apanicker-nflx\xa0 Can you please shed some light on this problem with respect to the new `TaskRunnerConfigurer` client?""]",0,0,0.9813699660142418,0.002535435149659239,0.0015927812133761095,0.013774576066340546,0
['@mdepak +1. Can you also add this to es6-persistence module to keep both the modules consistent please?'],0.9255127182793121,0.07403243285285807,0,0.0002655864920236542,0,8.949399767645217e-05,0
['@apanicker-nflx my build failed because LoggingMetricsModuleTest verify() did not pass its test. This has nothing to do with my change as far as I can tell. What should I do?'],0.0006805787628191625,0.9992579491560042,0,0,4.344047273136073e-05,0,0
['@mactaggart @mashurex Could you please help with reviewing this pull request? Thanks'],0.9992633056326417,0.0006894330257269365,0,0,0,0,0
[],0,0,0,0,0,0,0
"['@apanicker-nflx Could you please review my additions? I would also like to add another implementation for the `SemaphoreDAO` that implements a ""fair semaphore"" ([read more](https://redislabs.com/ebook/part-2-core-concepts/chapter-6-application-components-in-redis/6-3-counting-semaphores/6-3-2-fair-semaphores/)) for when setting up Conductor in HA and the clocks on different nodes Conductor is running on are not synchronized and I\'m unsure of how to get an instance of the configuration here https://github.com/Netflix/conductor/blob/d4c550f38020ba58b312cf2299361af294ebd3f0/redis-persistence/src/main/java/com/netflix/conductor/dao/RedisWorkflowModule.java#L31-L36 to make this configurable by the user ([in my repo](https://github.com/TwoUnderscorez/conductor/blob/8dc12c76c4f07f0d27c1a5574c96f48b5453535f/redis-persistence/src/main/java/com/netflix/conductor/dao/RedisWorkflowModule.java#L42)). I\'ll be glad if you could point me in the right direction because this is basically my first time writing anything in Java.']",0.005786843500747613,0.9554924088710335,0,0.00023191305951538816,0,0.03841805298681126,0
['@mashurex @jvemugunta @demichej Could somebody please review this?\\r\\nThanks!'],0.9992542565050018,0.0007281019342465327,0,5.754519568428423e-06,0,0,0
"['Hey @TwoUnderscorez, Can you provide more details about how this ties together with [Task concurrent execution limits ](https://netflix.github.io/conductor/configuration/taskdef/#task-concurrent-execution-limits) please?']",2.0826777904292926e-05,1.2696984927103387e-05,0,0,0,0.9999585627785453,0
"[""> @kishorebanala I'm not entirely sure what you mean by how it ties together, but the order of checks in `exceedsInProgressLiint` are as follows:\\r\\n> \\r\\n> 1. `TaskDef` concurrent execution limit\\r\\n> 2. `WorkflowTask` execution limit (what I have added)\\r\\n> 3. `TaskDef`rate limit check\\r\\n>\xa0 \xa0 Checks 1 and 2 are completely separate checks.\\r\\n\\r\\n@TwoUnderscorez Given that Conductor already provides Task Concurrent execution limits and Task rate limits which is currently in use, how does this new addition fit into to the existing terminology? How can a new user discern between all the concurrency limit constructs available?\\r\\n\\r\\nCan you provide a concrete example when the existing concurrency limits wouldn't be helpful, and what additional use cases this new feature would help with please?""]",2.8917026039606382e-05,0.9999633307373339,0,0,0,0,0
['@kishorebanala Any update here? Thanks.'],0.9564660066914629,0.005170046997901117,0.030087333793286358,0.006954458599408125,0,0,0
['Hey @mashurex @jvemugunta Can you help review these MySQL / Postgres module changes please?'],0.9997011670094755,0.00027879801469398883,0,0,0,0,0
['@demichej @jvemugunta Can you review this PR? Thanks.'],0.9997309521398686,0.00023749285954861594,0,0,0,9.428891675374078e-06,0
['Thank you for the contribution.\\r\\n\\r\\n@rickfish @mactaggart @mashurex Could you please help with reviewing this PR? Thank you.'],0.9990514740354187,0.0009246964837246015,0,0,0,0,0
"['I have a general question on the scope of the commit. If this is around setting up Flyway to do baseline migrations (which I think is a fantastic idea), why are there incoming changes to DAO classes and tests?']",0.0004939506226114213,0.9830726452389422,0,4.9074598460263576e-05,0.01626645934592381,0,0
[],0,0,0,0,0,0,0
['@Yingsheng-eroad is this still valid?'],0.907266941136676,0.00012063103119632912,0,0.09253018161077071,0,0,0
['@rickfish Can you please review this?'],0.7341510876977636,0.2655675354067443,0,0.00012592110473817522,0,0,0
"[""@marosmars, this looks good and I am glad you are using the lock/share attributes. I wanted to do that but it is in the guts of the queue dao. we have so much volume and so many polling operations that I am a little scared to introduce it into high-volume environments where we might miss race conditions and get killed in production. In any case, it looks good on the surface but I will have to dig into it heavily and actually deploy to one of our lower environments that has significant volume. I can't do that until after next week because we are all working on a production deployment on the 13th of the month. One question: why the double query for LOCK_TASKS in the processUnacks methods?""]",0.06489675217842768,0.5993346054908131,0,0,0.16320581597261735,0.14194179496830897,0
"[""@rickfish You can use the PerformanceTest (https://github.com/Netflix/conductor/pull/1940/files#diff-8a372fc5d838b1a09cac3814bb9cb8df1819e64b726eb2cd2988743b199b35f8) in this PR to stress test the Queue Dao. It is not production env, but it can simulate a lot of load with multiple producers/consumers and thus approximate your production env. I was using that while developing this PR. The test by default spawns 4 producer threads, 8 consumer threads (workers) and expects a set number of messages to be pushed into the queue and successfully processed. In addition, there's a few threads invoking queueDetail() and a few additional threads invoking processUnacks() to put additional load on the dao. The test is a unit test, but expects a real PSQL to be deployed manually on the host. In addition to the attached performance test (testing just the Queue Dao) I was running a performance test executing workflows against a full conductor + psql + 8 worker nodes setup. This external test also behaved well and the performance was higher with this PR applied. So I totally understand your concerns, but the testing mentioned above should increase confidence in this PR a bit Note: I was actually running into problems in the queue dao without this PR when executing that performance test ... e.g. https://github.com/Netflix/conductor/pull/1940/files#diff-fab744a4a59813bc19b455dbc4f34e34e3657655dde025e0177f9621dfca72f4 > One question: why the double query for LOCK_TASKS in the processUnacks methods? That's a mistake, thanks for pointing it out. Will remove the double query.""]",0.02226354703815212,0.05039783808317185,0,0,0.0023676454468678824,0.9228287665998102,0
['@rickfish thank you for review and testing. @apanicker-nflx can we get this in ?'],0.9995145104382145,0.00021434321912792105,4.773162379938142e-06,0.00025468883826639416,0,0,0
['@rickfish Does this change fix the issue #1806?'],0.9625604399870191,0.03510442107111993,0,0,0,0.0017681389963710116,0
['@mdepak Can you please pick this back up and drive to completion as it seems to be affecting other users as well? Thanks.'],0.9975049847375281,0.002355091732601446,0,4.8700366608927946e-05,0,0,0
"['@apanicker-nflx\xa0 The intent of this pull request is to ensure that the default behavior is maintained before my previous changes (https://github.com/Netflix/conductor/pull/1748) of introducing Execuution scheudler. The following version of Obervable.interval() method was invoked earlier from the ObservableQueue implementations and it is using compuation scheudler. ```public static Observable<Long> interval(long initialDelay, long period, TimeUnit unit) {return interval(initialDelay, period, unit, Schedulers.computation());}```With this PR, when the config (`Configuration#getEventSchedulerPollThreadCount()`) is not provided (NULL), `Schedulers.computation()` is returned thus maintining the default behavior. I have verified with 100 types of event queues in my local with Schedulers.computation() scheduler and it works as expected. Whereas dedicated threadpool helps in high througput scenairos where each thread is allocated for one event type.\xa0 Could we merge this PR so that existing behavior is maintained before I dig deep to identify root cause to fix in Executor scheduler area as well?\xa0 Please let me know your thoughts on this. Sidenote: Ideally, the obervables from interval() should share the threads from the passed scheduler object to poll from the queues. This happends as expected for Schedulers.computation(). In theory,Executor scheduler is expected to behave same way. But a thread from the Executor scheudler pool always polls a particular queue type instead of relinquishing it after a single call. This causes only few event types to be processed. RxJava version (1.2.2) is deprecated and I guess no support is available. I just started trying to migrate to latest RxJava 3.0.0 version to see if any bug is fixed. Apologies for the dealyed respones.']",0.0005594913447207264,0.9993546093541943,0,2.8979206569520673e-05,0,0,0
"['This looks good to me. Just to have another pair of eyes, preferably from Postgres module users, can one of you help reviewing this? @raghavendra-bhat, @marosmars , @rickfish.. Thank you!']",0.9999772771531678,1.2987840296944958e-05,0,0,0,0,0
"[""@mactaggart I have a query, in general, on the use of 'id' columns in the tables. I think you might have a reason in using but I don't see this column being used in any of the execution and queue dao related tables queries (except for one instance where one of the queries use ordering by Id in task_in_progress table but even that could be replaced by created_on?). @kishorebanala @rickfish""]",0.3801279509159782,0.6089229123441112,0,0.0009129449270992642,0.009690434636087299,0,0
"['Looking at this comment Netflix/spectator#782, I think that something is missing (in fact, exporting the metrics as in this PR, make a lot of label/tags unavailable in Prometheus)\\r\\n\\r\\n@manan164 could you please advise on how to handle Prometheus metrics?']",0.04794542951128478,0.3400243657394869,0,0,0.6024499063870198,0.009046475903821037,0
['How do you restart build in TravisCI? We are also trying to contribute to the project with my colleague @azb96 by merging PR #2038 @andrea11 ?'],0.00026949519421515207,1.7869503735420744e-05,0,0,0,0.9996761493709394,0
[],0,0,0,0,0,0,0
['@andrea11 Is there any way we can communicate with you privately about the contribution process for guidance?'],0.9900919335432761,0,0,0.00838395096550992,0.0002515071850613894,0.0011489019716346748,0
['@apanicker-nflx could you please review this PR and eventually merge it? Thank you'],0.9990258845587847,0.0005931442107995272,0,0,0,0.00022302604491124405,0
['@rickfish @raghavendra-bhat could you please help with reviewing this PR?'],0.9950219263835142,0.004848502425315691,0,0,0,2.7510687291679822e-05,0
['@rickfish Can you please verify the changes to the postgres module?\\r\\n@mashurex Can you help reviewing the changes to the mysql module?'],0.38618125777785184,0.6131736341837248,0,0.0001747435652730736,0,0,0
['@apanicker-nflx \\r\\nCan you please review this PR? \\r\\nThanks'],0.8271084879788779,0.17237293890180966,0,0.00010407643259008242,0,0.00029827490401583936,0
"[""I don't see 'es7-persistence' folder in main branch.\xa0 any plans to make it available ?""]",0.00010128725186390047,0.00696870547207192,0,7.907945851388276e-06,0,0,0
[],0,0,0,0,0,0,0
"[""Our build is failing with this error. Can you help us please? \\r\\n\\r\\n@kishorebanala @apanicker-nflx @aravindanr @jfernandez\\r\\n\\r\\n```\\r\\ntoomanyrequests: You have reached your pull rate limit. You may increase the limit by authenticating and upgrading: https://www.docker.com/increase-rate-limit\\r\\nNo stopped containers\\r\\n> Task :conductor-mysql-persistence:integrationTestDockerComposeUp FAILED\\r\\nFAILURE: Build failed with an exception.\\r\\n* What went wrong:\\r\\nExecution failed for task ':conductor-mysql-persistence:integrationTestDockerComposeUp'.\\r\\n> Exit-code 1 when calling docker-compose, stdout: N/A\\r\\n```""]",0.02253055254461621,0.9769598069875263,0,7.02533896267406e-05,0,0.00042171577069908736,0
['@kishorebanala could you please review our PR?'],0.00010820434881330698,0.0006465166538944011,0,0,0,0,0
"['Hello @kishorebanala,\\r\\n\\r\\nWe have completed all of your requested changes. Could you please review our PR again?']",0.3679963350375099,0.6316631167944414,0,0.00014120553979487505,0,0,0
"[""@moustaki : not sure who to contact, but is the dev branch also included for the next 3.0 version?\\r\\n\\r\\nI'd be happy to have this fix in both versions.""]",0.9964627463737561,0.0035163456368313972,0,9.37545925600911e-06,0,0,0
"[""Hello @Jiehong how did you manage to restart your build? Ours stuck in the dockercomposeup stage and we couldn't restart our build.""]",0.05168087440755988,0.09339136405127464,0.0002753834686940192,0,0,0.8540053334119605,0
[],0,0.12845212656210414,0,0,0,0,0
['@Jiehong thank you. Is there any way we can communicate with you privately about the contribution process for guidance?'],0.13984259644385977,0.00036957997494801846,0,0.8546193105651005,0,0.004737969550135231,0
"[""@azb96 : I don't know much more than you, as I'm not working at Netflix. @kishorebanala : would you have more info regarding @azb96 's request? Also, feel free to approve this ;)""]",0.9999720323251027,0,0,8.427933098989742e-06,0,0,0
"[""Hi @rickfish \\r\\nThanks for the review. \\r\\nFlyway changed the default schema history table name from `schema_version` to `flyway_schema_history`.\\r\\nSo, it's not recognized your history table and you got an error.\\r\\nI updated the configurations to use `schema_version` as default.\\r\\ncan you pls try to run it again with the new commit? \\r\\n""]",0.017475162358572275,0.9804968127386746,0,0,0.0002184352338398066,0.0016879385288998403,0
"['Hi @rickfish ,\\r\\nAny update? Did you get a chance to look into it?']",0.8206491709832292,0.017007781574164906,0.12939584592752423,0,0,0.028101916559608947,0
"['Hi @apanicker-nflx / @aravindanr, \\r\\nCould you pls review and merge this PR?\\r\\nThis issue blocking us, and force us to work with a forked repo.\\r\\nWe have a requirement to destroy and deploy new conductor clusters which consist of multiple instances, \\r\\nWhen we deploy a new cluster with an empty database we get the following error:\\r\\nERROR: duplicate key value violates unique constraint ""pg_type_typname_nsp_index""\\r\\nThis PR fixes this issue, by updating the flyway version (which causes this bug).\\r\\n\\r\\n\\r\\n']",0.016291020704214183,0.9836221348294582,0,1.0615801067943149e-05,0,5.9453147058787695e-05,0
"[""@apanicker-nflx When do you think someone could have a look? I personally think it's almost done. The only things that are missing are integ tests and potentially in memory tests on some of the ExecutionDAO tables. I don't think the in memory tables should be configurable, as it introduces a lot of complexities and is probably a niche feature anyway. So just testing general compatibility is fine and then letting the users define the in memory table on the sqlserver is probably a better approach.""]",0.00014626814663412006,0,0.9993363941837995,0,0,0.0002928218951954259,0
"[""@apanicker-nflx I've done some cursory review at this point but my SQL Server is a bit rusty compared to MySQL and PostgreSQL.@mactaggart @rickfish @s50600822 now that there are going to be 3 different RDBMS persistence layers, do you think there would be an advantage to unifying them to some degree? I have lots of thoughts about the schema and some issues I've noticed in the MySQL layer over time. @apanicker-nflx should I create an issue for the thought? Pull request?""]",0.05570266430884283,0.9441034329838868,0,0.00010245836879376155,0,4.3077682602857736e-05,0
"['@apanicker-nflx @mactaggart @rickfish @mashurex\xa0 \xa0 what are your opinions on using stored procedures? We are currently running about 50,000 WFs per day and the performance is either like MySql or little bit better at times which is pretty good. We will be running about 10 times as much in the future though. I asked our DBA what he thinks and he basically said that we should use stored procedures. The SQL Server does undesirable things with the queue_message table which sometimes cause the pop to take longer than 1 second. And these things can only be adjusted with stored procedures. Also, do you prefer I merge to version 2 or 3? There are a lot of changes in 3 so that might take longer to figure out. Thanks']",0.00014193008227018954,4.791673149523e-07,0,0,0,0,0
"["">Up until v3.0.2 it seemed like conductor.db.type was just ignored and the in memory redis/dynomite was used, and on v3.0.3 I'm getting some kind of a Bean exception when trying to use MySQL.\\r\\n\\r\\n@TwoUnderscorez This is addressed in the latest release [v3.0.4](https://github.com/Netflix/conductor/releases/tag/v3.0.4). Could you please try again? Since 3.0.x is the version with long-term support, we recommend that SQL server persistence is added to the `main` with `2.31` as the next choice.""]",0,0.23981605188323735,0,0.27486970323925397,0.10277949611386274,0.2930977523024751,0
"[""@apanicker-nflx @mactaggart @rickfish @mashurex  @aravindanr I have made a few more changes, can you please have a look? As for conductor 3, I'll open a seprate pull request soon. Also, do you have any idea why the integration tests for sqlserver are failing? They are working locally and I'm clearly setting the maxPoolSize [here](https://github.com/TwoUnderscorez/conductor/blob/29ac43e541dbd6afc707b6d4562654aece14d016/test-harness/src/test/java/com/netflix/conductor/tests/utils/SqlServerTestRunner.java#L54)""]",0.9828094726013991,0.01641154382498872,0.000247044271637158,0.0003896023322289009,0,0,0
"[""Hi @apanicker-nflx / @aravindanr \\r\\nThis PR not reviewed for a long time, do you think this feature/fix still relevant? (I see that the search API still returns WorkflowSummary instead of Workflow).\\r\\nFrom our side, it will be much easier to get Workflow and not WorkflowSummary and cast it.\\r\\nif it's relevant I will update the PR with the latest changes (main branch).""]",0.02651541545069627,0.973400423022973,0,0,3.7227158345518305e-05,0,0
['Was this PR opened by mistake on the source instead of the fork?'],0.0012168332398029556,0.995714520685839,0,0.002958863609707509,0,0,0
['@apanicker-nflx @aravindanr\xa0 Could you take a look at this?'],0.9990657504830818,0.0005018477856943706,0,0.00042218890580639706,0,0,0
"['@apanicker-nflx Hi, the build failed because an ExternalPayloadStorage test which not related to my changes.Error: com.netflix.conductor.test.integration.ExternalPayloadStorageSpec > Test conditional workflow with system task using external payload storage FAILED Can you please assist? Thanks']",0.10549895830510751,0.8929521258616578,0,0,0.0009469439059063406,0.0002228142707772193,0
"['@apanicker-nflx \\r\\nHi\xa0 the check ""coverage/coveralls â€” Coverage decreased (-0.1%) to 70.329%"" failed, but is not related to what i added in the PR, what do you suggest to do?']",0.0001494015366265765,0.9998406931906776,0,0,0,0,0
"['> @apanicker-nflx\\r\\n> Hi, the build failed because an ExternalPayloadStorage test which not related to my changes.\\r\\n> Error:\\r\\n> com.netflix.conductor.test.integration.ExternalPayloadStorageSpec > Test conditional workflow with system task using external payload storage FAILED\\r\\n> \\r\\n> Can you please assist?\\r\\n> \\r\\n> Thanks\\r\\n\\r\\nWe are aware of this flaky test in v2.x. Restarting the build fixes it. We are looking to fix this forward in v3.0\\r\\n\\r\\n>@apanicker-nflx\\r\\n>Hi the check ""coverage/coveralls â€” Coverage decreased (-0.1%) to 70.329%"" failed, but is not related to what i added in the PR, what do you suggest to do?\\r\\n\\r\\nCoveralls has been unreliable partly due to the fact that it is not fully configured for the project, you can ignore this.']",0.006114958478330922,0.9922502404410195,0,0,0.0011767227911280644,0.0003236745572192351,0
['@maheshyaddanapudi Could you please break this PR into two parts - one which adds Spring Security and another for docker file changes? This would make it easier to review and also also cleanly separate out the changes. Thanks'],0.0027362565919822448,0.9967080597561649,0,0,0,0.0005189756982893327,0
"['> @maheshyaddanapudi Could you please break this PR into two parts - one which adds Spring Security and another for docker file changes? This would make it easier to review and also also cleanly separate out the changes. Thanks\\r\\n\\r\\n@aravindanr I understand. There are a few commits which have both the changes. Hence the easiest or fast way to segregate them for me, would be to delete the forked repo, first checkin the security changes and submit a PR. Once that is done, I can add Docker changes and create another. Does that sound ok ?']",0.14341817721906722,0.6914543126365168,0.0016123976244871152,0,0,0.16317441819418738,0
"[""@maheshyaddanapudi Thanks for README updates. This PR can't be merged since it contains security changes. Can you create a PR with just updates to the README file?""]",0.130673399813816,0.8644537711120078,0,0,0.004235679563985696,0.00019885453110948104,0
[],0,0,0,0,0,0,0
"['@apanicker-nflx While I understand where you\'re coming from, I unfortunately won\'t be able to do what you\'ve suggested.F337I personally think that the SQL Server is one of the most feature-rich modules and therefore should be in the main repo.* It supports fully all the workflow DAOs* Can act as a decider lock service* Highly configurable and documented* The SQL is written cooperatively with a DBA with 20+ years of experience.* It can be set up in any of these modes with only an SQL Server (no other technologies required): 1. Solo conductor with solo SQL Server (1 queue zone / no decider lock required). 2. Conductor per zone with HA (handled by Conductor) and DR (handled by SQL Server always-on cluster). a. 1 Conductor queue shard, 1\xa0 locking namespaces -> A cluster of Conductors with them dynamically assigning work depending on how big / busy each environment is. Any workflow started by any Conductor can be run by any other Conductor instance/zone. b. N Conductor queue shards, no decider lock required -> A cluster of Conductor but all workflows are local to the Conductor they were started by. A custom scheduling service is probably required for this which will devide the workload. 3. The ultimate setup - given an SQL Server always-on cluster running on 3 regions, and you have a k8s cluster on each region, deploy 2 Conductor instances on each k8s, both of the Conductor instances in each region share a queue shard and a locking namespace. Why 2 Conductor instances? In case one of them goes down and k8s is having trouble bringing it back up. We also wrote an external scheduling microservice which will do the right thing if a whole region goes down. It\'s not ready yet, so till then we\'re using 2a. While Dynomite is deprecated and is extremly difficult to set up.MySQL/Postgres - no way to devide queues by zone / zookeeper/redis is required for HA and it\'s not really documented Cassandra - verry little documentation and no full implemntation. So just redis is fully working? I haven\'t checked, but I think that sqlserver is either as feature-rich as redis or richer.If you decide to not accept this, I will just close my PRs and we\'ll just maintain an internal fork. I am not a Java developer and I don\'t want to be. I don\'t know how to do what you\'ve described and unfortunately, I don\'t have the time and patience to learn how. Also please note, this is not yet updated with version 2 of the PR, that\'s why it\'s still marked as a draft. You should probably not look at my code here until I mark it as ""Ready for review"". I will ask you to reconsider the idea of merging this module to your repo and update me, thanks.']",0.015124898285703646,0.7366039886785647,0,0.24061171159627026,0,0.006804517153920495,0
"['@FpyrypT I\'ve tried your branch with AWS ES7, and works well except for ""logs"" . Have you seen this before? \\r\\n\\r\\n` \xa0 \xa0 \\r\\n \xa0 \xa0 \xa0 at java.lang.Thread.run(Thread.java:834) [?:?]\\r\\n185826 [http-nio-8080-exec-8] ERROR com.netflix.conductor.es7.dao.index.']",0.041929044753212956,0.9572246385952445,0,0.00010256477579624992,0,0.0007110045704371842,0
"[""@aravindanr, I've added shaded es7-persistence module. Can you review the changes?""]",0.918090468181483,0.08181939179172654,0,2.625103870487441e-05,0,0,0
"['@aravindanr hi, when do you think you will be able to merge this PR? Thanks']",0,0,0.9994267587655735,9.966649456657116e-05,0,0,0
[],0,0.12845212656210414,0,0,0,0,0
"['Added a comment, merged the changes and got the same error on the build. Not used to rebasing so I added the comment. What is the process for merging to 3.x? What branch do I merge to?']",3.889507196770043e-05,0.9999535564435353,0,0,0,0,0
"['@apanicker-nflx created a pull request for 3.x, how to get around the io.chaossystems.grpc:grpc-healthcheck:1.0.1 build error for 2.31?']",6.375347236069976e-06,4.993038078989091e-06,0,0,0,0.9999766811478886,0
['any plan on merging this one?'],0.004613460743644835,0.9945041805766125,0,7.909333082154756e-05,0,0.0006192050995594773,0
"['@aravindanr thanks for pointing me to the discussion tab. I never noticed it I guess. I\'ll use it for next time.Regarding your points, perhaps they can be compatible: - If the task is FAILED already, then ""retryIfJqIstrue"" is not evaluated (aka today\'s behaviour) - If not (like COMPLETED) and the number of retries is not exhausted and retryIfJqIstrue is not null, then ""retryIfJqIstrue"" is evaluated its output is checked: - if ""true"": task is set to FAILED, with a message (""set for retry because of retryIfJqIstrue""), and the normal retry mecanism is used- if anything other than ""true"": status is not changed, and a message is added (something like ""retryIfJqIstrue did not evaluate to false"") What do you think?']",0.011090038566871326,0.3248447185197549,0,0,0.0013697418329526377,0.6625681887521371,0
['@apanicker-nflx can you review this RP?'],0.8159468670886322,0.18301352898381767,0,0.00034945306400540775,0,0,0
"['@ritu-p , can you please review this PR?']",0.9966724153225417,0.0032526117174147876,0,0,0,0,0
"['@aravindanr\xa0 @apanicker-nflx As part of this PR i\'m upgrading the version of the amqp-client since the current version (5.9.0) contains an issue related to consumer recovery and consumer tags. Since I updated the relevant dependencies.lock files, the compilation creates the final jar with the right amqp-client version, but if i delete the dependencies.lock files and run ""_./gradlew generateLock updateLock saveLock_"" to recreate them, the lock files of the **server** and **test-harness** will have the older version 5.9.0 and not 5.13.0. This situation will cause that the final boot jar will have the amp-client:5.9.0 and not 5.13.0. Updating the following files fix the problem: 1. Add to **./server/build.gradle**_implementation( group: \'com.rabbitmq\', name: \'amqp-client\'){ version{require ""${revAmqpClient}""}}_ 2. Add to **./test-harness/build.gradle** _testImplementation( group: \'com.rabbitmq\', name: \'amqp-client\'){ version{require ""${revAmqpClient}""}}_ Are you ok with this approach or you have a better idea? Thanks']",0.0046707061914865265,0.9696866770648102,0,1.97339116220571e-05,0,0.025595810326789977,0
['@ritu-p @aravindanr \\r\\nI made the requested changes and update the unit-test.\\r\\nCan you please review and merge if everything is ok?\\r\\nThanks'],0.9934053521644262,0.006570761812574403,0,5.138882061067896e-06,0,0,0
['@sajid-moinuddin Can you add some details about the changes in the PR?'],0.9990578940607238,0.0008554465026654531,0,0,0,1.949883183668547e-05,0
"['@kishorebanala @apanicker-nflx @aravindanr \\r\\nCan you please review this PR and raise concerns, if any?']",0.9997192770300687,0.0002592299635633743,0,0,0,0,0
['@ritu-p can you please help review this PR?'],0.9895286463798052,0.010422587009574843,0,0,0,0,0
"['>Are there any other places I should add tests? The workflow integration tests seem to be the most logical, but please let me know!\\r\\n\\r\\nParametersUtilsTest and WorkflowAndTaskConfigurationSpec provide good coverage for this change. Thanks for adding the tests.']",0.0006149905230200397,0.9990964510464085,0,0.0002604613698733639,0,1.8727753312207378e-05,0
['Is there something else that needs to be done here?'],0.0004286518101239777,0.9992906806859688,0,0,0.0002093621563651706,2.186008908440069e-05,0
"['> We have deployed this fix to our production server already, and it works as expected. How does this get back-ported to Conductor V2?\\r\\n\\r\\nThanks for taking the time to fix this bug. There is no automatic backport to 2.31. If you need this in that, please create another PR with the same changes targeting the `2.31` branch.']",0.0009097920509559778,0.0030675247897308877,0.001991046299053369,0,0,0.9937575800692947,0
[],0,0,0,0,0,0,0
['Do you have prints of the changes?'],0.0006570413525394791,0.11073855680002356,0,0.8884088902294026,0,0,0
['@aravindanr can we get this through? Current go client is not compatible with v3'],0.20610721178894106,0.7837587130246179,0,0.006784267162521821,0,0.0021911450892497905,0
['@taojwmware can you execute the following gradle command and commit the changes?`./gradlew generateLock saveLock`'],0.07242273569143452,0.0015177610856162592,1.9573753427104726e-05,0,0,0.9259903273656154,0
[],0,0.12845212656210414,0,0,0,0,0
['@aravindanr any advice here? Not sure how to kick off the above build issue?'],0.9218928840450822,0.033563745417163386,0,0.00766691853399886,0,0.03027360101073797,0
['@apanicker-nflx any insight here? Not sure if this has to do with the 2.31 travis changes in terms of the above build failure?'],0.04399825521740539,0.7299002193529024,0.03565555228246645,0,0.14646699002144506,0,0
['@apanicker-nflx or @aravindanr ? Any help here?'],0.9999437822452772,1.8602994417769285e-05,0,0,0,0,0
['@james-deee Sorry about the delay. This slipped through the cracks. The Github workflow is not working correctly. Could you rebase from `Netflix:2.31` into your fork and run the checks again?'],0.008833266506399075,0.015060437598910006,0.8944324767735385,0,0,0.07321797705380305,0
"[""@aravindanr this build failed. Is it a change to using Java 11 over here in 2.31 land? I bet it used to compile with Java 8? There's now a failure it looks like: ``` > Task :conductor-contribs:javadoc ^\xa0 symbol: \xa0 class PreDestroy location: package javax.annotation /home/runner/work/conductor/conductor/contribs/src/main/java/com/netflix/conductor/contribs/listener/ArchivingWithTTLWorkflowStatusListener.java:53: error: cannot find symbol 2 errors @PreDestroy```https://github.com/Netflix/conductor/runs/4157426509?check_suite_focus=true""]",0.8775368238685833,0.12214376822212507,0,6.93424004333794e-05,0,0.0002003058719391564,0
['Looks like it. I switched the ci build to use Java 8. Can you rebase and try again?'],0.0014512413287377465,0.9967850450708968,0.0002785694940282172,0,0,0.0014471613948386709,0
['@aravindanr that did it. Can you merge this when you get a chance? Thanks.'],6.756962569027768e-05,4.0017648046037914e-05,5.5596739528553556e-05,0,0,0,0
['@jxu-nflx Thanks for approving it. Could you please help merge it? Thanks!'],0.9995949114105473,0,1.6855610318434443e-05,2.0487901843600844e-05,0,0.00031708521600741713,0
['Seems the code change in this PR is unrelated to the failed test. Is it possible to rerun the check?'],3.689723285902607e-05,2.3732171098081645e-05,0,0,0,7.954307119245058e-06,0
"['Hey @rickfish ,\\r\\n\\r\\nI believe you use psql db for conductor and you helped us review this PR related to psql in the past: https://github.com/Netflix/conductor/pull/1940\\r\\n\\r\\nCould you take a look at this one as well ?\\r\\nAnd would this feature be useful to you ?']",0.9997410780860659,0.00024187052394625906,0,0,4.203092475176573e-06,0,0
"['Hey @rickfish thanks for taking a look and also for the info regarding performance of PostgresExecutionDAO.getWorkflowsByCorrelationId().\\r\\n\\r\\nHey @apanicker-nflx , can we get a review on this ?']",0.984246367295751,0.015514742159626551,0,3.927913877493764e-05,0,0,0
"['@Vasyl-9 Could you add Postgres payload storage as a separate module, like `azureblob-storage`?']",0.1483397393184494,0.8491607841302762,0,0.0022098160548855556,0,0,0
['After a discussion with @aravindanr it appears this entire block of code is entirely unnecessary as `domain` is evaluated as part of the `tasks` array where it is available and we should just use the value as-is returned by the API. \\r\\n\\r\\nCould you try if the following simplification works?\\r\\n\\r\\n\\r\\n```\\r\\n\xa0 const handleSelectedTask = (task) => {\\r\\n\xa0 \xa0 \xa0 \xa0 setSelectedTask(task)'],1.4657374064884633e-05,3.425222927661228e-06,0,0,0,9.200027424461031e-05,0
"['@apanicker-nflx , could you please merge this as we have got the required approvals?']",0.6656987620491075,0.2819208277843954,0.04738975087154996,0,0,0.0028873822014529966,0
['Wondering is this PR ready to be reviewed or still work in progress?'],0.0001467290308141197,1.9113665644021583e-05,0,0,0,0,0
['Anyone can do a favor for this commit? thanks a lot'],0.9999083813732428,4.206792433262952e-05,0,0,0,1.2225312017807552e-05,0
"[""Any reason why this PR was closed, @james-deee? I believe it's important. Also, I believe you need to change not only the version in graddle, but you would need also to rerun the gradlew locking the versions: ```./gradlew generateLock saveLock```. Last, but not least, there is still vulnerability with log4j 2.16, so we should go to log4j version 2.17. Please, let me know if you can address those points.""]",0.02479821014470923,0.08813992239878378,0,0,0.8802286902390205,0.005972197672322005,0
"[""> Any reason why this PR was closed, @james-deee? I believe it's important. Also, I believe you need to change not only the version in graddle, but you would need also to rerun the gradlew locking the versions: `./gradlew generateLock saveLock`. Last, but not least, there is still vulnerability with log4j 2.16, so we should go to log4j version 2.17. Please, let me know if you can address those points.\\r\\n\\r\\n@flavioschuindt isn't conductor version 2 using log4j1? Which is not subject to the same problem""]",0.0005522316190313712,0.9868238199885061,0,0,0.011828058868006764,0.00048066370037967287,0
['Thanks for merging @apanicker-nflx. Do you have any idea of when 2.31.6 will be released with this fix in place? Thank you.'],0,0,5.652406959486594e-05,0,0,0,0
"['@apanicker-nflx @aravindanr @kishorebanala I would like to discuss follow up actions with the community here: Above fix, of course, will address 2.31 branch. This means next version in this branch (maybe 2.31.7) will be ok. However, there are multiple versions in the 2.X range. In fact, just on my team, we have different versions being used at same time in production (e.g.: 2.27.2 & 2.30.3). Would be good if we had a way to backport this PR in others 2.X versions. I totally understand that is quite challenging as there are multiple versions released, but any idea how we should address it? If this PR is merged, I will open a PR soon to address my other two releases that are in production, but like I said there are other different releases that I am not even consuming on my application, but others might be using. Please, let me know your thoughts.']",0.5863958239143156,0.40726903184805785,0,0.0014275658327276637,0,0.004341119564353931,0
['@flavioschuindt we are also waiting for this fix. Can anyone please merge it? Thanks!'],0.9993510392661646,0.0004959393088081447,0,4.344208005858719e-05,0,0,0
['We also need this fix urgently. Is there anything holding it up? Thanks.'],0,0,0.0005985608304214342,0,0.32051118648538035,0.004948785925798443,0
"['@aravindanr Hi, I have rewritten a PR about PostgreSQL external storage. Can we get a review on this?']",0.34848217425628125,0.6513559447934272,0,0,0,0,0
['I reran this again locally on my system and the build was successful. Is this a problem with then environment?\\r\\n\\r\\n@apanicker-nflx @aravindanr @kishorebanala'],0.22265860496856985,0.5507091652187022,0,0.03727043104834109,0,0.17256500032652392,0
"[""@venkag We do not see a significant value gain by adding this retry logic feature, specifically since there isn't a requirement from the community. Could you please provide more details on why this needs to be added to Conductor and what specific usecases you are trying to address here? Also, please note that as part of the [contributing guidelines](https://github.com/Netflix/conductor/blob/main/CONTRIBUTING.md#i-want-to-contribute), we recommend starting a [discussion](https://github.com/Netflix/conductor/discussions) to gather inputs from the community and gauge interest. This also helps to finalize the implementation details for a contribution.""]",0.0002109778302317821,0.00045663201968105177,0,0,0.998807390025206,0.0004564205110660876,0
"[""What's the driving factor behind this change, if you don't mind my asking? DTO/DO separation tends to add a lot of complexity in my experience.""]",0.003946003438473738,0.06811495407443804,0,0,0.9263134432320811,0,0
[],0,0,0,0,0,0,0
"['Should `HTTP` task be in its own module, like `jsonjq-task`? Adding `HTTP` to core bloats it with ""web"" related dependencies.']",0.00011746924539023624,0.999759419894036,0,2.8378109564240088e-05,0,7.509151411894803e-05,0
[],0,0,0,0,0,0,0
[],0,0,0,0,0,0,0
['> If you want to use IAM client auth you need this lib so it is used only if you add the IAM authentication in the request which declare the class in this lib \\r\\n\\r\\nCan you add this as a `runtimeOnly` dependency in that case?'],0.01617866305899023,0.6377526798057581,0,0,0.017417427387638903,0.32764135457623295,0
['> @manan164 could you add more details about the bug that is fixed in this PR?\\r\\n\\r\\nHi @aravindanr\xa0 Added details.'],0.8082247687497145,0.19173748656071832,0,8.195314249597739e-06,0,0,0
"['@dougsillars Since this documentation is published under Conductor OSS, we cannot link to Orkes playground/code blocks from it. Would it be possible to use a local Conductor server and UI to run the code lab?']",0.006481142969433349,0.0445702358661192,0,0.9418585706422145,0.006575751571489719,0,0
"['@apanicker-nflx I realized I actually started without the `-d` flag:```docker run -p 8080:8080 -t conductor:server```When you run without `-d` do you see normal output or the exception I mentioned? And can you get a response from `curl http://localhost:8080/api/metadata/workflow`? When I run with `-d` the container starts but not healthily and I get no response when running `curl http://localhost:8080/api/metadata/workflow`. If I build the container with the fix in this PR I can see a healthy start without starting as a daemon, and I can run that curl and get the workflow definition back.']",0,0.021207069201209242,0.9715981324448575,0.002816416350148725,0,0.003579846702561142,0
"['@CherishSantoshi There are no changes here, is this intentional?']",0.5369585368629668,0.1594687022738635,0.001683549359871451,0.30125126349855297,0,0,0
"['Yes, moved all changes to RELATED.md as suggested by @aravindanr in the first comment.\\r\\nRaised another PR for the same - https://github.com/CherishSantoshi/conductor/pull/2\\r\\nHave I missed anything?']",0.10326282931429402,0.8962844678102313,0.00010156607358414078,0.0002951759319194509,0,0,0
"['Hello @v1r3n currently the Cassandra module requires an uuid for workflow and tasks: https://github.com/Netflix/conductor/blob/main/cassandra-persistence/src/main/java/com/netflix/conductor/cassandra/dao/CassandraBaseDAO.java#L152\\r\\nSo if a custom ID generator is used, user needs to be aware of that might not work with Cassandra module. Could you please add comment or document it somewhere?\\r\\n\\r\\nAlso, the id generator is all over the code bases, would you be interested to refactor that so to move everything to the storage layer?']",0.02466408603800544,0.8585250416466086,0,0,0.002097805669179517,0.11450831941283189,0
[],0,0.08909194968912915,0,0,0,0,0
"['@v1r3n After upgrading Conductor to Spring Boot 2.6.6, `java-sdk` tests are failing. Can you look into it?\\r\\n\\r\\nhttps://github.com/Netflix/conductor/runs/6085953694?check_suite_focus=true']",0.39382186153841836,0.605860934664021,9.683553828640084e-05,8.869092367354751e-05,0,0,0
"['@apanicker-nflx - I have created a new PR #2964 to fix the problem, I was unable to edit this PR. Can we mark this PR as wont fix and merge #2964 instead?']",0.6746089427960789,0.12742672314883816,0.15341015388955323,0,0,0.040524927515444925,0
"['@apanicker-nflx sure - I\'ll add some documentation.What would be the best place for it and its extent?The only thing I was able to find related to `conductor.redis.hosts` is a comment in `application.properties````#format is host:port:rack separated by semicolonconductor.redis.hosts=host1:port:rack;host2:port:rack:host3:port:rack```Would it be enough just adding to that? or would you prefer having something like ""Configuring Redis"" under `/docs/how-tos` and an entry in `spring-configuration-metadata.json`?']",0.01226028625563392,0.9815883824258129,0,0.0008776058351846548,0,0.005141469733595476,0
"['A comment in `application.properties` would help devs.\\r\\nAdditionally, a section in the docs to elaborate would provide more clarity. @dougsillars Could you point to the best place for adding this documentation?']",0.023639311765560344,0.20361195051136471,0,0.77039294971529,0,0.0018633819306008196,0
['@v1r3n can you update the documentation section for `WAIT` task? cc: @dougsillars'],0.9993436570873038,0.00013740703206132824,0,0.00047977323153340043,0,0,0
['@v1r3n Could you please add a documentation entry for this task? cc: @dougsillars'],0.9712224171833205,0.027834147074812462,0,0.000786025439722246,0,8.693533426437195e-05,0
['@aravindanr Can you please help to review it? Thanks.'],0.999503209991041,0.0004592946108959358,0,0,0,7.1948504723652375e-06,0
['@v1r3n could you take a look at this PR?'],0.9994618860352804,0.0004556666711816097,0,1.979068867132478e-05,0,0,0
"[""Just my 2 cents here, but loosening this type to a String doesn't seem quite like the correct thing to do here. Wonder if there's a better thing we can do to achieve your goal.\\r\\n\\r\\n@manan164 can you give everyone a more descriptive, in depth explanation of what you are trying to get by doing this? I can't quite tell by your comment above.""]",0.00038628929511554046,0.9995930049449665,0,0,0,0,0
['Hi @aravindanr\xa0 @apanicker-nflx any update on this please?'],0.9550700312864567,0,0.03257359787093962,0.010072372492009945,0,0.0011639999181392476,0
['@jxu-nflx Would you please kindly advise If this PR fixed only switch task or applicable for all kinds of tasks? i.e. fixed the issue #3089 completely?'],0.006787947947169326,0.9926935243277711,0,0,0,0.0002723150200151211,0
[],0,0,0,0,0,0,0
[],0,0,0,0,0,0,0
[],0,0,0,0,0,0,0
"['@manan164, thank you for the fix. Does anything prevent merging of this fix?\\r\\n']",0.5146873757596908,0.46819845209839667,0,0,0,0.015069401225695877,0
"['nit: Title and description of this PR to be modified to maybe ""fix scheduleTime of task""?']",0.0005737168362517348,6.294520323374826e-05,0,0,0,0,0
['Can you add a test for the changes?'],1.2684384258496053e-06,5.682336807874366e-06,0,0,0,0,0
"[""@apanicker-nflx If this looks ok to you can you approve the workflow? Also I have the doc changes for https://github.com/Netflix/conductor/issues/2975 staged, I don't see them added yet. Can I add them as well along with the current change?""]",0.7085235933375434,0.01745375268277893,0.25532914536727785,0.009143293676046386,0,0,0
"[""@ghoshabhi and @apanicker-nflx when is an ETA that we can expect this to get merged and for a new release of Conudctor to go out? I know there is also some batch polling that was added to the Java client that we'd also like to take advantage of. Thanks!""]",1.8815360008127783e-05,0,0.9998212662747757,0,0,0,0
['@jxu-nflx When will the next release happen ?\\r\\nWe want to leverage batch polling support.'],0,0,0.9998334229554968,0,0,0,0
['> @apanicker-nflx Requesting your review\\r\\n\\r\\n@apanicker-nflx Can you help with review ?'],0.717149860329253,0.2826789436718965,0,0,0,7.132169493775287e-05,0
"['Hi @charybr , thanks for reporting. \\r\\nFor \\r\\n1. Is possible for you to share the definition? May be the condition is in such a way that it executes only one iteration or some parsing error.\\r\\n2. Are you trying to put task output in external storage? The conductor has a limit on that. By default, it is 10MB.\\r\\nLet me know if this works, or we can chat [here](https://join.slack.com/t/orkes-conductor/shared_invite/zt-xyxqyseb-YZ3hwwAgHJH97bsrYRnSZg) for more realtime collaboration.']",0.004038653236043452,0.9830593964581794,0,0.00038701852417297406,0,0.011957997232003428,0
"['Hi @aravindanr , can you please review the changes? cc @AvitalOfstein']",0.9995720448589358,0.0004082721792383683,0,0,0,0,0
"[""@manan164 , i got the following error when starting conductor on the last main branch, how can I configure `MeterRegistry`?```[com/netflix/conductor/core/config/ConductorCoreConfiguration.class]: Unsatisfied dependency expressed through method 'getEventQueueProviders' parameter 0; nested exception is org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'conductorEventQueueProvider' defined in URL [jar:file:/Users/xxx/workspace/platform/conductor/core/build/libs/conductor-core-3.14.0-SNAPSHOT.jar!/com/netflix/conductor/core/events/queue/ConductorEventQueueProvider.class]: Unsatisfied dependency expressed through constructor parameter 0; nested exception is org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'getQueueDAOStandalone' defined in class path resource [io/orkes/conductor/queue/config/RedisQueueConfiguration.class]: Unsatisfied dependency expressed through method 'getQueueDAOStandalone' parameter 1; nested exception is org.springframework.beans.factory.NoSuchBeanDefinitionException: No qualifying bean of type 'io.micrometer.core.instrument.MeterRegistry' available: expected at least 1 bean which qualifies as autowire candidate. Dependency annotations: {}```the relevant snippet of `application.properties`:```conductor.queue.type=redis_standalone conductor.redis.hosts=redis.xxx.amazonaws.com:6379:us-west-2b""]",0.040213223285297944,0,0,0.06996550108933752,0.00959423770295097,0.87617779176782,0
"['Hi @missedone , Thanks for reporting. Can you please share the properties used other than those mentioned above? I tried but could not be able to reproduce it.']",0.12187472574062394,0.875864290121243,0,0.0014907175466677043,0.00030676210900299167,0,0
[],0,0.12845212656210414,0,0,0,0,0
"['Hi @missedone , I tried with above properties but looks like server works fine.\\r\\nI see you are using postgres and queue so I have added `implementation ""com.netflix.conductor:conductor-postgres-persistence:3.13.3""` in server/build.gradle. Rest all things kept same. Can you please join [community](https://join.slack.com/t/orkes-conductor/shared_invite/zt-xyxqyseb-YZ3hwwAgHJH97bsrYRnSZg)? We can debug together.']",0.2003078792179347,0.021019119564909927,0,0.06177862797359271,0,0.7053244988573515,0
['@sourabh30 can you resolve the conflicts?'],0.9993380581802147,0.0005939637306367081,0,0,0,0,0
['@bjpirt can you resolve the conflicts?'],0.9947743753294461,0.005017407063182269,0,0,0,0,0
"['Hi @astelmashenko , Changes looks good. Can you please add a small unit test around this? Please check [reference](core/src/test/java/com/netflix/conductor/core/events/TestSimpleActionProcessor.java)']",0.08917404117211797,0.8884303455631738,0.00016536622469596446,0,0,0.021994795979317312,0
"['@manan164 , I quickly checked the reference test. DefaultEventQueueProcessor is wirtten a way it will require a lot of ceremony to setup a test. To make it easier to test DefaultEventQueueProcessor should be refactored e.g. into testable service with logic to process events and processor with code that subscribes to observable/etc. What do you think about the reafactoring?']",5.237684744108343e-06,1.1154607608290107e-06,0,0,0,0,0
"['@manan164 , why do not we merge this?']",0.003049450099084849,0,0,0,0.9963569298411501,0,0
"['Hey @v1r3n , can we get a quick look on this small fix ?\\r\\n\\r\\nThx, Maros']",0.9953567639061844,0.004495414507819007,7.225500211085619e-05,4.719450932497849e-05,0,0,0
